{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e03d4f-9ae5-464b-ab29-45e56cf0f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Found 9920 validated image filenames belonging to 6 classes.\n",
      "Found 2480 validated image filenames belonging to 6 classes.\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/310 ━━━━━━━━━━━━━━━━━━━━ 23:59:56\n",
      "Accuracy: 0.4062 - Loss: 1.7511\n",
      "\n",
      "Batch 2/310 ━━━━━━━━━━━━━━━━━━━━ 00:00:47\n",
      "Accuracy: 0.3750 - Loss: 1.7549\n",
      "\n",
      "Batch 3/310 ━━━━━━━━━━━━━━━━━━━━ 00:01:37\n",
      "Accuracy: 0.3542 - Loss: 1.7538\n",
      "\n",
      "Batch 4/310 ━━━━━━━━━━━━━━━━━━━━ 00:02:27\n",
      "Accuracy: 0.3281 - Loss: 1.7598\n",
      "\n",
      "Batch 5/310 ━━━━━━━━━━━━━━━━━━━━ 00:03:16\n",
      "Accuracy: 0.3375 - Loss: 1.7876\n",
      "\n",
      "Batch 6/310 ━━━━━━━━━━━━━━━━━━━━ 00:04:05\n",
      "Accuracy: 0.3281 - Loss: 1.7785\n",
      "\n",
      "Batch 7/310 ━━━━━━━━━━━━━━━━━━━━ 00:04:54\n",
      "Accuracy: 0.3214 - Loss: 1.7853\n",
      "\n",
      "Batch 8/310 ━━━━━━━━━━━━━━━━━━━━ 00:05:42\n",
      "Accuracy: 0.3281 - Loss: 1.7841\n",
      "\n",
      "Batch 9/310 ━━━━━━━━━━━━━━━━━━━━ 00:06:31\n",
      "Accuracy: 0.3299 - Loss: 1.7733\n",
      "\n",
      "Batch 10/310 ━━━━━━━━━━━━━━━━━━━━ 00:07:19\n",
      "Accuracy: 0.3438 - Loss: 1.7547\n",
      "\n",
      "Batch 11/310 ━━━━━━━━━━━━━━━━━━━━ 00:08:08\n",
      "Accuracy: 0.3466 - Loss: 1.7449\n",
      "\n",
      "Batch 12/310 ━━━━━━━━━━━━━━━━━━━━ 00:08:57\n",
      "Accuracy: 0.3516 - Loss: 1.7370\n",
      "\n",
      "Batch 13/310 ━━━━━━━━━━━━━━━━━━━━ 00:09:45\n",
      "Accuracy: 0.3606 - Loss: 1.7253\n",
      "\n",
      "Batch 14/310 ━━━━━━━━━━━━━━━━━━━━ 00:10:35\n",
      "Accuracy: 0.3594 - Loss: 1.7179\n",
      "\n",
      "Batch 15/310 ━━━━━━━━━━━━━━━━━━━━ 00:11:23\n",
      "Accuracy: 0.3521 - Loss: 1.7289\n",
      "\n",
      "Batch 16/310 ━━━━━━━━━━━━━━━━━━━━ 00:12:19\n",
      "Accuracy: 0.3633 - Loss: 1.7127\n",
      "\n",
      "Batch 17/310 ━━━━━━━━━━━━━━━━━━━━ 00:13:10\n",
      "Accuracy: 0.3640 - Loss: 1.7146\n",
      "\n",
      "Batch 18/310 ━━━━━━━━━━━━━━━━━━━━ 00:13:58\n",
      "Accuracy: 0.3681 - Loss: 1.7115\n",
      "\n",
      "Batch 19/310 ━━━━━━━━━━━━━━━━━━━━ 00:14:48\n",
      "Accuracy: 0.3651 - Loss: 1.7091\n",
      "\n",
      "Batch 20/310 ━━━━━━━━━━━━━━━━━━━━ 00:15:35\n",
      "Accuracy: 0.3688 - Loss: 1.7038\n",
      "\n",
      "Batch 21/310 ━━━━━━━━━━━━━━━━━━━━ 00:16:24\n",
      "Accuracy: 0.3720 - Loss: 1.6951\n",
      "\n",
      "Batch 22/310 ━━━━━━━━━━━━━━━━━━━━ 00:17:11\n",
      "Accuracy: 0.3778 - Loss: 1.6913\n",
      "\n",
      "Batch 23/310 ━━━━━━━━━━━━━━━━━━━━ 00:17:59\n",
      "Accuracy: 0.3791 - Loss: 1.6939\n",
      "\n",
      "Batch 24/310 ━━━━━━━━━━━━━━━━━━━━ 00:18:48\n",
      "Accuracy: 0.3854 - Loss: 1.6829\n",
      "\n",
      "Batch 25/310 ━━━━━━━━━━━━━━━━━━━━ 00:19:36\n",
      "Accuracy: 0.3875 - Loss: 1.6782\n",
      "\n",
      "Batch 26/310 ━━━━━━━━━━━━━━━━━━━━ 00:20:25\n",
      "Accuracy: 0.3906 - Loss: 1.6743\n",
      "\n",
      "Batch 27/310 ━━━━━━━━━━━━━━━━━━━━ 00:21:12\n",
      "Accuracy: 0.3912 - Loss: 1.6682\n",
      "\n",
      "Batch 28/310 ━━━━━━━━━━━━━━━━━━━━ 00:22:00\n",
      "Accuracy: 0.3951 - Loss: 1.6674\n",
      "\n",
      "Batch 29/310 ━━━━━━━━━━━━━━━━━━━━ 00:22:49\n",
      "Accuracy: 0.3955 - Loss: 1.6683\n",
      "\n",
      "Batch 30/310 ━━━━━━━━━━━━━━━━━━━━ 00:23:37\n",
      "Accuracy: 0.3969 - Loss: 1.6703\n",
      "\n",
      "Batch 31/310 ━━━━━━━━━━━━━━━━━━━━ 00:24:25\n",
      "Accuracy: 0.4012 - Loss: 1.6583\n",
      "\n",
      "Batch 32/310 ━━━━━━━━━━━━━━━━━━━━ 00:25:13\n",
      "Accuracy: 0.3984 - Loss: 1.6579\n",
      "\n",
      "Batch 33/310 ━━━━━━━━━━━━━━━━━━━━ 00:26:01\n",
      "Accuracy: 0.3987 - Loss: 1.6561\n",
      "\n",
      "Batch 34/310 ━━━━━━━━━━━━━━━━━━━━ 00:26:50\n",
      "Accuracy: 0.4017 - Loss: 1.6476\n",
      "\n",
      "Batch 35/310 ━━━━━━━━━━━━━━━━━━━━ 00:27:37\n",
      "Accuracy: 0.4045 - Loss: 1.6390\n",
      "\n",
      "Batch 36/310 ━━━━━━━━━━━━━━━━━━━━ 00:28:26\n",
      "Accuracy: 0.4036 - Loss: 1.6365\n",
      "\n",
      "Batch 37/310 ━━━━━━━━━━━━━━━━━━━━ 00:29:13\n",
      "Accuracy: 0.4037 - Loss: 1.6372\n",
      "\n",
      "Batch 38/310 ━━━━━━━━━━━━━━━━━━━━ 00:30:01\n",
      "Accuracy: 0.4005 - Loss: 1.6384\n",
      "\n",
      "Batch 39/310 ━━━━━━━━━━━━━━━━━━━━ 00:30:50\n",
      "Accuracy: 0.4022 - Loss: 1.6333\n",
      "\n",
      "Batch 40/310 ━━━━━━━━━━━━━━━━━━━━ 00:31:38\n",
      "Accuracy: 0.4055 - Loss: 1.6272\n",
      "\n",
      "Batch 41/310 ━━━━━━━━━━━━━━━━━━━━ 00:32:26\n",
      "Accuracy: 0.4078 - Loss: 1.6202\n",
      "\n",
      "Batch 42/310 ━━━━━━━━━━━━━━━━━━━━ 00:33:14\n",
      "Accuracy: 0.4055 - Loss: 1.6211\n",
      "\n",
      "Batch 43/310 ━━━━━━━━━━━━━━━━━━━━ 00:34:02\n",
      "Accuracy: 0.4062 - Loss: 1.6172\n",
      "\n",
      "Batch 44/310 ━━━━━━━━━━━━━━━━━━━━ 00:34:51\n",
      "Accuracy: 0.4070 - Loss: 1.6143\n",
      "\n",
      "Batch 45/310 ━━━━━━━━━━━━━━━━━━━━ 00:35:39\n",
      "Accuracy: 0.4042 - Loss: 1.6175\n",
      "\n",
      "Batch 46/310 ━━━━━━━━━━━━━━━━━━━━ 00:36:27\n",
      "Accuracy: 0.4042 - Loss: 1.6141\n",
      "\n",
      "Batch 47/310 ━━━━━━━━━━━━━━━━━━━━ 00:37:15\n",
      "Accuracy: 0.4056 - Loss: 1.6105\n",
      "\n",
      "Batch 48/310 ━━━━━━━━━━━━━━━━━━━━ 00:38:02\n",
      "Accuracy: 0.4049 - Loss: 1.6074\n",
      "\n",
      "Batch 49/310 ━━━━━━━━━━━━━━━━━━━━ 00:38:51\n",
      "Accuracy: 0.4037 - Loss: 1.6083\n",
      "\n",
      "Batch 50/310 ━━━━━━━━━━━━━━━━━━━━ 00:39:39\n",
      "Accuracy: 0.4044 - Loss: 1.6062\n",
      "\n",
      "Batch 51/310 ━━━━━━━━━━━━━━━━━━━━ 00:40:27\n",
      "Accuracy: 0.4038 - Loss: 1.6044\n",
      "\n",
      "Batch 52/310 ━━━━━━━━━━━━━━━━━━━━ 00:41:15\n",
      "Accuracy: 0.4075 - Loss: 1.6004\n",
      "\n",
      "Batch 53/310 ━━━━━━━━━━━━━━━━━━━━ 00:42:03\n",
      "Accuracy: 0.4068 - Loss: 1.5989\n",
      "\n",
      "Batch 54/310 ━━━━━━━━━━━━━━━━━━━━ 00:42:51\n",
      "Accuracy: 0.4051 - Loss: 1.6001\n",
      "\n",
      "Batch 55/310 ━━━━━━━━━━━━━━━━━━━━ 00:43:39\n",
      "Accuracy: 0.4051 - Loss: 1.5993\n",
      "\n",
      "Batch 56/310 ━━━━━━━━━━━━━━━━━━━━ 00:44:27\n",
      "Accuracy: 0.4051 - Loss: 1.6009\n",
      "\n",
      "Batch 57/310 ━━━━━━━━━━━━━━━━━━━━ 00:45:15\n",
      "Accuracy: 0.4062 - Loss: 1.5969\n",
      "\n",
      "Batch 58/310 ━━━━━━━━━━━━━━━━━━━━ 00:46:03\n",
      "Accuracy: 0.4073 - Loss: 1.5957\n",
      "\n",
      "Batch 59/310 ━━━━━━━━━━━━━━━━━━━━ 00:46:51\n",
      "Accuracy: 0.4073 - Loss: 1.5974\n",
      "\n",
      "Batch 60/310 ━━━━━━━━━━━━━━━━━━━━ 00:47:39\n",
      "Accuracy: 0.4078 - Loss: 1.5978\n",
      "\n",
      "Batch 61/310 ━━━━━━━━━━━━━━━━━━━━ 00:48:27\n",
      "Accuracy: 0.4062 - Loss: 1.6014\n",
      "\n",
      "Batch 62/310 ━━━━━━━━━━━━━━━━━━━━ 00:49:15\n",
      "Accuracy: 0.4073 - Loss: 1.5989\n",
      "\n",
      "Batch 63/310 ━━━━━━━━━━━━━━━━━━━━ 00:50:03\n",
      "Accuracy: 0.4067 - Loss: 1.5993\n",
      "\n",
      "Batch 64/310 ━━━━━━━━━━━━━━━━━━━━ 00:50:51\n",
      "Accuracy: 0.4106 - Loss: 1.5951\n",
      "\n",
      "Batch 65/310 ━━━━━━━━━━━━━━━━━━━━ 00:51:39\n",
      "Accuracy: 0.4091 - Loss: 1.5964\n",
      "\n",
      "Batch 66/310 ━━━━━━━━━━━━━━━━━━━━ 00:52:28\n",
      "Accuracy: 0.4110 - Loss: 1.5973\n",
      "\n",
      "Batch 67/310 ━━━━━━━━━━━━━━━━━━━━ 00:53:16\n",
      "Accuracy: 0.4109 - Loss: 1.5967\n",
      "\n",
      "Batch 68/310 ━━━━━━━━━━━━━━━━━━━━ 00:54:03\n",
      "Accuracy: 0.4113 - Loss: 1.5949\n",
      "\n",
      "Batch 69/310 ━━━━━━━━━━━━━━━━━━━━ 00:54:52\n",
      "Accuracy: 0.4112 - Loss: 1.5949\n",
      "\n",
      "Batch 70/310 ━━━━━━━━━━━━━━━━━━━━ 00:55:40\n",
      "Accuracy: 0.4121 - Loss: 1.5946\n",
      "\n",
      "Batch 71/310 ━━━━━━━━━━━━━━━━━━━━ 00:56:28\n",
      "Accuracy: 0.4107 - Loss: 1.5988\n",
      "\n",
      "Batch 72/310 ━━━━━━━━━━━━━━━━━━━━ 00:57:16\n",
      "Accuracy: 0.4084 - Loss: 1.5996\n",
      "\n",
      "Batch 73/310 ━━━━━━━━━━━━━━━━━━━━ 00:58:03\n",
      "Accuracy: 0.4088 - Loss: 1.5984\n",
      "\n",
      "Batch 74/310 ━━━━━━━━━━━━━━━━━━━━ 00:58:52\n",
      "Accuracy: 0.4092 - Loss: 1.5994\n",
      "\n",
      "Batch 75/310 ━━━━━━━━━━━━━━━━━━━━ 00:59:39\n",
      "Accuracy: 0.4108 - Loss: 1.5959\n",
      "\n",
      "Batch 76/310 ━━━━━━━━━━━━━━━━━━━━ 01:00:27\n",
      "Accuracy: 0.4112 - Loss: 1.5943\n",
      "\n",
      "Batch 77/310 ━━━━━━━━━━━━━━━━━━━━ 01:01:15\n",
      "Accuracy: 0.4103 - Loss: 1.5960\n",
      "\n",
      "Batch 78/310 ━━━━━━━━━━━━━━━━━━━━ 01:02:03\n",
      "Accuracy: 0.4111 - Loss: 1.5952\n",
      "\n",
      "Batch 79/310 ━━━━━━━━━━━━━━━━━━━━ 01:02:51\n",
      "Accuracy: 0.4134 - Loss: 1.5935\n",
      "\n",
      "Batch 80/310 ━━━━━━━━━━━━━━━━━━━━ 01:03:39\n",
      "Accuracy: 0.4145 - Loss: 1.5903\n",
      "\n",
      "Batch 81/310 ━━━━━━━━━━━━━━━━━━━━ 01:04:27\n",
      "Accuracy: 0.4144 - Loss: 1.5893\n",
      "\n",
      "Batch 82/310 ━━━━━━━━━━━━━━━━━━━━ 01:05:15\n",
      "Accuracy: 0.4173 - Loss: 1.5848\n",
      "\n",
      "Batch 83/310 ━━━━━━━━━━━━━━━━━━━━ 01:06:02\n",
      "Accuracy: 0.4164 - Loss: 1.5832\n",
      "\n",
      "Batch 84/310 ━━━━━━━━━━━━━━━━━━━━ 01:06:51\n",
      "Accuracy: 0.4174 - Loss: 1.5812\n",
      "\n",
      "Batch 85/310 ━━━━━━━━━━━━━━━━━━━━ 01:07:38\n",
      "Accuracy: 0.4180 - Loss: 1.5796\n",
      "\n",
      "Batch 86/310 ━━━━━━━━━━━━━━━━━━━━ 01:08:26\n",
      "Accuracy: 0.4175 - Loss: 1.5799\n",
      "\n",
      "Batch 87/310 ━━━━━━━━━━━━━━━━━━━━ 01:09:15\n",
      "Accuracy: 0.4167 - Loss: 1.5800\n",
      "\n",
      "Batch 88/310 ━━━━━━━━━━━━━━━━━━━━ 01:10:02\n",
      "Accuracy: 0.4169 - Loss: 1.5806\n",
      "\n",
      "Batch 89/310 ━━━━━━━━━━━━━━━━━━━━ 01:10:50\n",
      "Accuracy: 0.4175 - Loss: 1.5799\n",
      "\n",
      "Batch 90/310 ━━━━━━━━━━━━━━━━━━━━ 01:11:38\n",
      "Accuracy: 0.4191 - Loss: 1.5766\n",
      "\n",
      "Batch 91/310 ━━━━━━━━━━━━━━━━━━━━ 01:12:27\n",
      "Accuracy: 0.4203 - Loss: 1.5744\n",
      "\n",
      "Batch 92/310 ━━━━━━━━━━━━━━━━━━━━ 01:13:15\n",
      "Accuracy: 0.4195 - Loss: 1.5748\n",
      "\n",
      "Batch 93/310 ━━━━━━━━━━━━━━━━━━━━ 01:14:03\n",
      "Accuracy: 0.4210 - Loss: 1.5719\n",
      "\n",
      "Batch 94/310 ━━━━━━━━━━━━━━━━━━━━ 01:14:51\n",
      "Accuracy: 0.4215 - Loss: 1.5709\n",
      "\n",
      "Batch 95/310 ━━━━━━━━━━━━━━━━━━━━ 01:15:39\n",
      "Accuracy: 0.4220 - Loss: 1.5698\n",
      "\n",
      "Batch 96/310 ━━━━━━━━━━━━━━━━━━━━ 01:16:26\n",
      "Accuracy: 0.4232 - Loss: 1.5665\n",
      "\n",
      "Batch 97/310 ━━━━━━━━━━━━━━━━━━━━ 01:17:15\n",
      "Accuracy: 0.4243 - Loss: 1.5637\n",
      "\n",
      "Batch 98/310 ━━━━━━━━━━━━━━━━━━━━ 01:18:03\n",
      "Accuracy: 0.4247 - Loss: 1.5648\n",
      "\n",
      "Batch 99/310 ━━━━━━━━━━━━━━━━━━━━ 01:18:51\n",
      "Accuracy: 0.4255 - Loss: 1.5647\n",
      "\n",
      "Batch 100/310 ━━━━━━━━━━━━━━━━━━━━ 01:19:38\n",
      "Accuracy: 0.4259 - Loss: 1.5639\n",
      "\n",
      "Batch 101/310 ━━━━━━━━━━━━━━━━━━━━ 01:20:27\n",
      "Accuracy: 0.4261 - Loss: 1.5625\n",
      "\n",
      "Batch 102/310 ━━━━━━━━━━━━━━━━━━━━ 01:21:15\n",
      "Accuracy: 0.4265 - Loss: 1.5605\n",
      "\n",
      "Batch 103/310 ━━━━━━━━━━━━━━━━━━━━ 01:22:03\n",
      "Accuracy: 0.4257 - Loss: 1.5621\n",
      "\n",
      "Batch 104/310 ━━━━━━━━━━━━━━━━━━━━ 01:22:51\n",
      "Accuracy: 0.4252 - Loss: 1.5627\n",
      "\n",
      "Batch 105/310 ━━━━━━━━━━━━━━━━━━━━ 01:23:39\n",
      "Accuracy: 0.4244 - Loss: 1.5641\n",
      "\n",
      "Batch 106/310 ━━━━━━━━━━━━━━━━━━━━ 01:24:27\n",
      "Accuracy: 0.4257 - Loss: 1.5620\n",
      "\n",
      "Batch 107/310 ━━━━━━━━━━━━━━━━━━━━ 01:25:16\n",
      "Accuracy: 0.4261 - Loss: 1.5608\n",
      "\n",
      "Batch 108/310 ━━━━━━━━━━━━━━━━━━━━ 01:26:03\n",
      "Accuracy: 0.4256 - Loss: 1.5602\n",
      "\n",
      "Batch 109/310 ━━━━━━━━━━━━━━━━━━━━ 01:26:51\n",
      "Accuracy: 0.4269 - Loss: 1.5579\n",
      "\n",
      "Batch 110/310 ━━━━━━━━━━━━━━━━━━━━ 01:27:39\n",
      "Accuracy: 0.4259 - Loss: 1.5599\n",
      "\n",
      "Batch 111/310 ━━━━━━━━━━━━━━━━━━━━ 01:28:27\n",
      "Accuracy: 0.4262 - Loss: 1.5591\n",
      "\n",
      "Batch 112/310 ━━━━━━━━━━━━━━━━━━━━ 01:29:16\n",
      "Accuracy: 0.4275 - Loss: 1.5563\n",
      "\n",
      "Batch 113/310 ━━━━━━━━━━━━━━━━━━━━ 01:30:04\n",
      "Accuracy: 0.4287 - Loss: 1.5549\n",
      "\n",
      "Batch 114/310 ━━━━━━━━━━━━━━━━━━━━ 01:30:52\n",
      "Accuracy: 0.4282 - Loss: 1.5556\n",
      "\n",
      "Batch 115/310 ━━━━━━━━━━━━━━━━━━━━ 01:31:39\n",
      "Accuracy: 0.4285 - Loss: 1.5549\n",
      "\n",
      "Batch 116/310 ━━━━━━━━━━━━━━━━━━━━ 01:32:27\n",
      "Accuracy: 0.4286 - Loss: 1.5547\n",
      "\n",
      "Batch 117/310 ━━━━━━━━━━━━━━━━━━━━ 01:33:16\n",
      "Accuracy: 0.4292 - Loss: 1.5527\n",
      "\n",
      "Batch 118/310 ━━━━━━━━━━━━━━━━━━━━ 01:34:04\n",
      "Accuracy: 0.4290 - Loss: 1.5525\n",
      "\n",
      "Batch 119/310 ━━━━━━━━━━━━━━━━━━━━ 01:34:52\n",
      "Accuracy: 0.4299 - Loss: 1.5502\n",
      "\n",
      "Batch 120/310 ━━━━━━━━━━━━━━━━━━━━ 01:35:40\n",
      "Accuracy: 0.4299 - Loss: 1.5489\n",
      "\n",
      "Batch 121/310 ━━━━━━━━━━━━━━━━━━━━ 01:36:27\n",
      "Accuracy: 0.4316 - Loss: 1.5478\n",
      "\n",
      "Batch 122/310 ━━━━━━━━━━━━━━━━━━━━ 01:37:16\n",
      "Accuracy: 0.4331 - Loss: 1.5448\n",
      "\n",
      "Batch 123/310 ━━━━━━━━━━━━━━━━━━━━ 01:38:04\n",
      "Accuracy: 0.4337 - Loss: 1.5419\n",
      "\n",
      "Batch 124/310 ━━━━━━━━━━━━━━━━━━━━ 01:38:52\n",
      "Accuracy: 0.4330 - Loss: 1.5419\n",
      "\n",
      "Batch 125/310 ━━━━━━━━━━━━━━━━━━━━ 01:39:40\n",
      "Accuracy: 0.4335 - Loss: 1.5410\n",
      "\n",
      "Batch 126/310 ━━━━━━━━━━━━━━━━━━━━ 01:40:28\n",
      "Accuracy: 0.4333 - Loss: 1.5406\n",
      "\n",
      "Batch 127/310 ━━━━━━━━━━━━━━━━━━━━ 01:41:16\n",
      "Accuracy: 0.4336 - Loss: 1.5396\n",
      "\n",
      "Batch 128/310 ━━━━━━━━━━━━━━━━━━━━ 01:42:04\n",
      "Accuracy: 0.4338 - Loss: 1.5389\n",
      "\n",
      "Batch 129/310 ━━━━━━━━━━━━━━━━━━━━ 01:42:52\n",
      "Accuracy: 0.4341 - Loss: 1.5376\n",
      "\n",
      "Batch 130/310 ━━━━━━━━━━━━━━━━━━━━ 01:43:40\n",
      "Accuracy: 0.4322 - Loss: 1.5395\n",
      "\n",
      "Batch 131/310 ━━━━━━━━━━━━━━━━━━━━ 01:44:27\n",
      "Accuracy: 0.4330 - Loss: 1.5379\n",
      "\n",
      "Batch 132/310 ━━━━━━━━━━━━━━━━━━━━ 01:45:16\n",
      "Accuracy: 0.4330 - Loss: 1.5377\n",
      "\n",
      "Batch 133/310 ━━━━━━━━━━━━━━━━━━━━ 01:46:04\n",
      "Accuracy: 0.4337 - Loss: 1.5370\n",
      "\n",
      "Batch 134/310 ━━━━━━━━━━━━━━━━━━━━ 01:46:52\n",
      "Accuracy: 0.4335 - Loss: 1.5368\n",
      "\n",
      "Batch 135/310 ━━━━━━━━━━━━━━━━━━━━ 01:47:39\n",
      "Accuracy: 0.4340 - Loss: 1.5361\n",
      "\n",
      "Batch 136/310 ━━━━━━━━━━━━━━━━━━━━ 01:48:27\n",
      "Accuracy: 0.4329 - Loss: 1.5372\n",
      "\n",
      "Batch 137/310 ━━━━━━━━━━━━━━━━━━━━ 01:49:15\n",
      "Accuracy: 0.4329 - Loss: 1.5358\n",
      "\n",
      "Batch 138/310 ━━━━━━━━━━━━━━━━━━━━ 01:50:03\n",
      "Accuracy: 0.4343 - Loss: 1.5335\n",
      "\n",
      "Batch 139/310 ━━━━━━━━━━━━━━━━━━━━ 01:50:51\n",
      "Accuracy: 0.4339 - Loss: 1.5329\n",
      "\n",
      "Batch 140/310 ━━━━━━━━━━━━━━━━━━━━ 01:51:38\n",
      "Accuracy: 0.4344 - Loss: 1.5324\n",
      "\n",
      "Batch 141/310 ━━━━━━━━━━━━━━━━━━━━ 01:52:26\n",
      "Accuracy: 0.4357 - Loss: 1.5295\n",
      "\n",
      "Batch 142/310 ━━━━━━━━━━━━━━━━━━━━ 01:53:14\n",
      "Accuracy: 0.4362 - Loss: 1.5274\n",
      "\n",
      "Batch 143/310 ━━━━━━━━━━━━━━━━━━━━ 01:54:02\n",
      "Accuracy: 0.4379 - Loss: 1.5250\n",
      "\n",
      "Batch 144/310 ━━━━━━━━━━━━━━━━━━━━ 01:54:50\n",
      "Accuracy: 0.4377 - Loss: 1.5250\n",
      "\n",
      "Batch 145/310 ━━━━━━━━━━━━━━━━━━━━ 01:55:38\n",
      "Accuracy: 0.4390 - Loss: 1.5221\n",
      "\n",
      "Batch 146/310 ━━━━━━━━━━━━━━━━━━━━ 01:56:25\n",
      "Accuracy: 0.4386 - Loss: 1.5227\n",
      "\n",
      "Batch 147/310 ━━━━━━━━━━━━━━━━━━━━ 01:57:13\n",
      "Accuracy: 0.4390 - Loss: 1.5213\n",
      "\n",
      "Batch 148/310 ━━━━━━━━━━━━━━━━━━━━ 01:58:01\n",
      "Accuracy: 0.4392 - Loss: 1.5215\n",
      "\n",
      "Batch 149/310 ━━━━━━━━━━━━━━━━━━━━ 01:58:49\n",
      "Accuracy: 0.4396 - Loss: 1.5210\n",
      "\n",
      "Batch 150/310 ━━━━━━━━━━━━━━━━━━━━ 01:59:37\n",
      "Accuracy: 0.4406 - Loss: 1.5192\n",
      "\n",
      "Batch 151/310 ━━━━━━━━━━━━━━━━━━━━ 02:00:25\n",
      "Accuracy: 0.4412 - Loss: 1.5176\n",
      "\n",
      "Batch 152/310 ━━━━━━━━━━━━━━━━━━━━ 02:01:13\n",
      "Accuracy: 0.4416 - Loss: 1.5165\n",
      "\n",
      "Batch 153/310 ━━━━━━━━━━━━━━━━━━━━ 02:02:00\n",
      "Accuracy: 0.4414 - Loss: 1.5157\n",
      "\n",
      "Batch 154/310 ━━━━━━━━━━━━━━━━━━━━ 02:02:49\n",
      "Accuracy: 0.4422 - Loss: 1.5147\n",
      "\n",
      "Batch 155/310 ━━━━━━━━━━━━━━━━━━━━ 02:03:37\n",
      "Accuracy: 0.4413 - Loss: 1.5152\n",
      "\n",
      "Batch 156/310 ━━━━━━━━━━━━━━━━━━━━ 02:04:24\n",
      "Accuracy: 0.4403 - Loss: 1.5160\n",
      "\n",
      "Batch 157/310 ━━━━━━━━━━━━━━━━━━━━ 02:05:13\n",
      "Accuracy: 0.4401 - Loss: 1.5158\n",
      "\n",
      "Batch 158/310 ━━━━━━━━━━━━━━━━━━━━ 02:06:00\n",
      "Accuracy: 0.4399 - Loss: 1.5161\n",
      "\n",
      "Batch 159/310 ━━━━━━━━━━━━━━━━━━━━ 02:06:48\n",
      "Accuracy: 0.4395 - Loss: 1.5160\n",
      "\n",
      "Batch 160/310 ━━━━━━━━━━━━━━━━━━━━ 02:07:37\n",
      "Accuracy: 0.4398 - Loss: 1.5148\n",
      "\n",
      "Batch 161/310 ━━━━━━━━━━━━━━━━━━━━ 02:08:24\n",
      "Accuracy: 0.4398 - Loss: 1.5142\n",
      "\n",
      "Batch 162/310 ━━━━━━━━━━━━━━━━━━━━ 02:09:12\n",
      "Accuracy: 0.4396 - Loss: 1.5134\n",
      "\n",
      "Batch 163/310 ━━━━━━━━━━━━━━━━━━━━ 02:10:01\n",
      "Accuracy: 0.4390 - Loss: 1.5134\n",
      "\n",
      "Batch 164/310 ━━━━━━━━━━━━━━━━━━━━ 02:10:48\n",
      "Accuracy: 0.4411 - Loss: 1.5105\n",
      "\n",
      "Batch 165/310 ━━━━━━━━━━━━━━━━━━━━ 02:11:38\n",
      "Accuracy: 0.4403 - Loss: 1.5112\n",
      "\n",
      "Batch 166/310 ━━━━━━━━━━━━━━━━━━━━ 02:12:25\n",
      "Accuracy: 0.4403 - Loss: 1.5107\n",
      "\n",
      "Batch 167/310 ━━━━━━━━━━━━━━━━━━━━ 02:13:14\n",
      "Accuracy: 0.4409 - Loss: 1.5099\n",
      "\n",
      "Batch 168/310 ━━━━━━━━━━━━━━━━━━━━ 02:14:02\n",
      "Accuracy: 0.4410 - Loss: 1.5091\n",
      "\n",
      "Batch 169/310 ━━━━━━━━━━━━━━━━━━━━ 02:14:49\n",
      "Accuracy: 0.4414 - Loss: 1.5074\n",
      "\n",
      "Batch 170/310 ━━━━━━━━━━━━━━━━━━━━ 02:15:39\n",
      "Accuracy: 0.4421 - Loss: 1.5058\n",
      "\n",
      "Batch 171/310 ━━━━━━━━━━━━━━━━━━━━ 02:16:26\n",
      "Accuracy: 0.4424 - Loss: 1.5045\n",
      "\n",
      "Batch 172/310 ━━━━━━━━━━━━━━━━━━━━ 02:17:14\n",
      "Accuracy: 0.4424 - Loss: 1.5032\n",
      "\n",
      "Batch 173/310 ━━━━━━━━━━━━━━━━━━━━ 02:18:02\n",
      "Accuracy: 0.4426 - Loss: 1.5028\n",
      "\n",
      "Batch 174/310 ━━━━━━━━━━━━━━━━━━━━ 02:18:50\n",
      "Accuracy: 0.4427 - Loss: 1.5020\n",
      "\n",
      "Batch 175/310 ━━━━━━━━━━━━━━━━━━━━ 02:19:39\n",
      "Accuracy: 0.4437 - Loss: 1.4999\n",
      "\n",
      "Batch 176/310 ━━━━━━━━━━━━━━━━━━━━ 02:20:26\n",
      "Accuracy: 0.4439 - Loss: 1.4988\n",
      "\n",
      "Batch 177/310 ━━━━━━━━━━━━━━━━━━━━ 02:21:14\n",
      "Accuracy: 0.4431 - Loss: 1.4991\n",
      "\n",
      "Batch 178/310 ━━━━━━━━━━━━━━━━━━━━ 02:22:02\n",
      "Accuracy: 0.4442 - Loss: 1.4978\n",
      "\n",
      "Batch 179/310 ━━━━━━━━━━━━━━━━━━━━ 02:22:50\n",
      "Accuracy: 0.4448 - Loss: 1.4963\n",
      "\n",
      "Batch 180/310 ━━━━━━━━━━━━━━━━━━━━ 02:23:39\n",
      "Accuracy: 0.4450 - Loss: 1.4956\n",
      "\n",
      "Batch 181/310 ━━━━━━━━━━━━━━━━━━━━ 02:24:26\n",
      "Accuracy: 0.4444 - Loss: 1.4956\n",
      "\n",
      "Batch 182/310 ━━━━━━━━━━━━━━━━━━━━ 02:25:14\n",
      "Accuracy: 0.4445 - Loss: 1.4946\n",
      "\n",
      "Batch 183/310 ━━━━━━━━━━━━━━━━━━━━ 02:26:02\n",
      "Accuracy: 0.4447 - Loss: 1.4945\n",
      "\n",
      "Batch 184/310 ━━━━━━━━━━━━━━━━━━━━ 02:26:49\n",
      "Accuracy: 0.4451 - Loss: 1.4935\n",
      "\n",
      "Batch 185/310 ━━━━━━━━━━━━━━━━━━━━ 02:27:38\n",
      "Accuracy: 0.4453 - Loss: 1.4929\n",
      "\n",
      "Batch 186/310 ━━━━━━━━━━━━━━━━━━━━ 02:28:26\n",
      "Accuracy: 0.4464 - Loss: 1.4917\n",
      "\n",
      "Batch 187/310 ━━━━━━━━━━━━━━━━━━━━ 02:29:14\n",
      "Accuracy: 0.4470 - Loss: 1.4900\n",
      "\n",
      "Batch 188/310 ━━━━━━━━━━━━━━━━━━━━ 02:30:02\n",
      "Accuracy: 0.4463 - Loss: 1.4903\n",
      "\n",
      "Batch 189/310 ━━━━━━━━━━━━━━━━━━━━ 02:30:49\n",
      "Accuracy: 0.4473 - Loss: 1.4891\n",
      "\n",
      "Batch 190/310 ━━━━━━━━━━━━━━━━━━━━ 02:31:38\n",
      "Accuracy: 0.4469 - Loss: 1.4895\n",
      "\n",
      "Batch 191/310 ━━━━━━━━━━━━━━━━━━━━ 02:32:25\n",
      "Accuracy: 0.4473 - Loss: 1.4882\n",
      "\n",
      "Batch 192/310 ━━━━━━━━━━━━━━━━━━━━ 02:33:14\n",
      "Accuracy: 0.4479 - Loss: 1.4881\n",
      "\n",
      "Batch 193/310 ━━━━━━━━━━━━━━━━━━━━ 02:34:01\n",
      "Accuracy: 0.4479 - Loss: 1.4877\n",
      "\n",
      "Batch 194/310 ━━━━━━━━━━━━━━━━━━━━ 02:34:48\n",
      "Accuracy: 0.4476 - Loss: 1.4865\n",
      "\n",
      "Batch 195/310 ━━━━━━━━━━━━━━━━━━━━ 02:35:37\n",
      "Accuracy: 0.4476 - Loss: 1.4860\n",
      "\n",
      "Batch 196/310 ━━━━━━━━━━━━━━━━━━━━ 02:36:25\n",
      "Accuracy: 0.4477 - Loss: 1.4851\n",
      "\n",
      "Batch 197/310 ━━━━━━━━━━━━━━━━━━━━ 02:37:13\n",
      "Accuracy: 0.4481 - Loss: 1.4847\n",
      "\n",
      "Batch 198/310 ━━━━━━━━━━━━━━━━━━━━ 02:38:01\n",
      "Accuracy: 0.4493 - Loss: 1.4829\n",
      "\n",
      "Batch 199/310 ━━━━━━━━━━━━━━━━━━━━ 02:38:48\n",
      "Accuracy: 0.4501 - Loss: 1.4819\n",
      "\n",
      "Batch 200/310 ━━━━━━━━━━━━━━━━━━━━ 02:39:36\n",
      "Accuracy: 0.4502 - Loss: 1.4815\n",
      "\n",
      "Batch 201/310 ━━━━━━━━━━━━━━━━━━━━ 02:40:23\n",
      "Accuracy: 0.4504 - Loss: 1.4802\n",
      "\n",
      "Batch 202/310 ━━━━━━━━━━━━━━━━━━━━ 02:41:11\n",
      "Accuracy: 0.4506 - Loss: 1.4797\n",
      "\n",
      "Batch 203/310 ━━━━━━━━━━━━━━━━━━━━ 02:41:59\n",
      "Accuracy: 0.4501 - Loss: 1.4800\n",
      "\n",
      "Batch 204/310 ━━━━━━━━━━━━━━━━━━━━ 02:42:47\n",
      "Accuracy: 0.4505 - Loss: 1.4789\n",
      "\n",
      "Batch 205/310 ━━━━━━━━━━━━━━━━━━━━ 02:43:35\n",
      "Accuracy: 0.4514 - Loss: 1.4773\n",
      "\n",
      "Batch 206/310 ━━━━━━━━━━━━━━━━━━━━ 02:44:23\n",
      "Accuracy: 0.4518 - Loss: 1.4765\n",
      "\n",
      "Batch 207/310 ━━━━━━━━━━━━━━━━━━━━ 02:45:10\n",
      "Accuracy: 0.4515 - Loss: 1.4767\n",
      "\n",
      "Batch 208/310 ━━━━━━━━━━━━━━━━━━━━ 02:45:58\n",
      "Accuracy: 0.4515 - Loss: 1.4765\n",
      "\n",
      "Batch 209/310 ━━━━━━━━━━━━━━━━━━━━ 02:46:45\n",
      "Accuracy: 0.4520 - Loss: 1.4753\n",
      "\n",
      "Batch 210/310 ━━━━━━━━━━━━━━━━━━━━ 02:47:33\n",
      "Accuracy: 0.4519 - Loss: 1.4753\n",
      "\n",
      "Batch 211/310 ━━━━━━━━━━━━━━━━━━━━ 02:48:21\n",
      "Accuracy: 0.4516 - Loss: 1.4758\n",
      "\n",
      "Batch 212/310 ━━━━━━━━━━━━━━━━━━━━ 02:49:10\n",
      "Accuracy: 0.4515 - Loss: 1.4758\n",
      "\n",
      "Batch 213/310 ━━━━━━━━━━━━━━━━━━━━ 02:49:58\n",
      "Accuracy: 0.4523 - Loss: 1.4744\n",
      "\n",
      "Batch 214/310 ━━━━━━━━━━━━━━━━━━━━ 02:50:45\n",
      "Accuracy: 0.4525 - Loss: 1.4733\n",
      "\n",
      "Batch 215/310 ━━━━━━━━━━━━━━━━━━━━ 02:51:33\n",
      "Accuracy: 0.4531 - Loss: 1.4724\n",
      "\n",
      "Batch 216/310 ━━━━━━━━━━━━━━━━━━━━ 02:52:21\n",
      "Accuracy: 0.4538 - Loss: 1.4702\n",
      "\n",
      "Batch 217/310 ━━━━━━━━━━━━━━━━━━━━ 02:53:09\n",
      "Accuracy: 0.4545 - Loss: 1.4687\n",
      "\n",
      "Batch 218/310 ━━━━━━━━━━━━━━━━━━━━ 02:53:56\n",
      "Accuracy: 0.4553 - Loss: 1.4673\n",
      "\n",
      "Batch 219/310 ━━━━━━━━━━━━━━━━━━━━ 02:54:44\n",
      "Accuracy: 0.4548 - Loss: 1.4674\n",
      "\n",
      "Batch 220/310 ━━━━━━━━━━━━━━━━━━━━ 02:55:32\n",
      "Accuracy: 0.4544 - Loss: 1.4673\n",
      "\n",
      "Batch 221/310 ━━━━━━━━━━━━━━━━━━━━ 02:56:19\n",
      "Accuracy: 0.4548 - Loss: 1.4669\n",
      "\n",
      "Batch 222/310 ━━━━━━━━━━━━━━━━━━━━ 02:57:07\n",
      "Accuracy: 0.4541 - Loss: 1.4673\n",
      "\n",
      "Batch 223/310 ━━━━━━━━━━━━━━━━━━━━ 02:57:55\n",
      "Accuracy: 0.4545 - Loss: 1.4664\n",
      "\n",
      "Batch 224/310 ━━━━━━━━━━━━━━━━━━━━ 02:58:43\n",
      "Accuracy: 0.4547 - Loss: 1.4666\n",
      "\n",
      "Batch 225/310 ━━━━━━━━━━━━━━━━━━━━ 02:59:31\n",
      "Accuracy: 0.4551 - Loss: 1.4656\n",
      "\n",
      "Batch 226/310 ━━━━━━━━━━━━━━━━━━━━ 03:00:18\n",
      "Accuracy: 0.4549 - Loss: 1.4660\n",
      "\n",
      "Batch 227/310 ━━━━━━━━━━━━━━━━━━━━ 03:01:06\n",
      "Accuracy: 0.4542 - Loss: 1.4665\n",
      "\n",
      "Batch 228/310 ━━━━━━━━━━━━━━━━━━━━ 03:01:55\n",
      "Accuracy: 0.4541 - Loss: 1.4666\n",
      "\n",
      "Batch 229/310 ━━━━━━━━━━━━━━━━━━━━ 03:02:42\n",
      "Accuracy: 0.4533 - Loss: 1.4668\n",
      "\n",
      "Batch 230/310 ━━━━━━━━━━━━━━━━━━━━ 03:03:30\n",
      "Accuracy: 0.4535 - Loss: 1.4659\n",
      "\n",
      "Batch 231/310 ━━━━━━━━━━━━━━━━━━━━ 03:04:18\n",
      "Accuracy: 0.4537 - Loss: 1.4651\n",
      "\n",
      "Batch 232/310 ━━━━━━━━━━━━━━━━━━━━ 03:05:06\n",
      "Accuracy: 0.4539 - Loss: 1.4643\n",
      "\n",
      "Batch 233/310 ━━━━━━━━━━━━━━━━━━━━ 03:05:55\n",
      "Accuracy: 0.4537 - Loss: 1.4639\n",
      "\n",
      "Batch 234/310 ━━━━━━━━━━━━━━━━━━━━ 03:06:43\n",
      "Accuracy: 0.4546 - Loss: 1.4629\n",
      "\n",
      "Batch 235/310 ━━━━━━━━━━━━━━━━━━━━ 03:07:31\n",
      "Accuracy: 0.4556 - Loss: 1.4609\n",
      "\n",
      "Batch 236/310 ━━━━━━━━━━━━━━━━━━━━ 03:08:19\n",
      "Accuracy: 0.4552 - Loss: 1.4617\n",
      "\n",
      "Batch 237/310 ━━━━━━━━━━━━━━━━━━━━ 03:09:06\n",
      "Accuracy: 0.4544 - Loss: 1.4629\n",
      "\n",
      "Batch 238/310 ━━━━━━━━━━━━━━━━━━━━ 03:09:55\n",
      "Accuracy: 0.4544 - Loss: 1.4626\n",
      "\n",
      "Batch 239/310 ━━━━━━━━━━━━━━━━━━━━ 03:10:43\n",
      "Accuracy: 0.4550 - Loss: 1.4614\n",
      "\n",
      "Batch 240/310 ━━━━━━━━━━━━━━━━━━━━ 03:11:31\n",
      "Accuracy: 0.4555 - Loss: 1.4603\n",
      "\n",
      "Batch 241/310 ━━━━━━━━━━━━━━━━━━━━ 03:12:18\n",
      "Accuracy: 0.4555 - Loss: 1.4598\n",
      "\n",
      "Batch 242/310 ━━━━━━━━━━━━━━━━━━━━ 03:13:06\n",
      "Accuracy: 0.4561 - Loss: 1.4581\n",
      "\n",
      "Batch 243/310 ━━━━━━━━━━━━━━━━━━━━ 03:13:55\n",
      "Accuracy: 0.4568 - Loss: 1.4567\n",
      "\n",
      "Batch 244/310 ━━━━━━━━━━━━━━━━━━━━ 03:14:43\n",
      "Accuracy: 0.4568 - Loss: 1.4567\n",
      "\n",
      "Batch 245/310 ━━━━━━━━━━━━━━━━━━━━ 03:15:31\n",
      "Accuracy: 0.4569 - Loss: 1.4565\n",
      "\n",
      "Batch 246/310 ━━━━━━━━━━━━━━━━━━━━ 03:16:19\n",
      "Accuracy: 0.4576 - Loss: 1.4552\n",
      "\n",
      "Batch 247/310 ━━━━━━━━━━━━━━━━━━━━ 03:17:07\n",
      "Accuracy: 0.4575 - Loss: 1.4548\n",
      "\n",
      "Batch 248/310 ━━━━━━━━━━━━━━━━━━━━ 03:17:56\n",
      "Accuracy: 0.4577 - Loss: 1.4543\n",
      "\n",
      "Batch 249/310 ━━━━━━━━━━━━━━━━━━━━ 03:18:44\n",
      "Accuracy: 0.4578 - Loss: 1.4535\n",
      "\n",
      "Batch 250/310 ━━━━━━━━━━━━━━━━━━━━ 03:19:32\n",
      "Accuracy: 0.4581 - Loss: 1.4527\n",
      "\n",
      "Batch 251/310 ━━━━━━━━━━━━━━━━━━━━ 03:20:20\n",
      "Accuracy: 0.4578 - Loss: 1.4525\n",
      "\n",
      "Batch 252/310 ━━━━━━━━━━━━━━━━━━━━ 03:21:07\n",
      "Accuracy: 0.4580 - Loss: 1.4520\n",
      "\n",
      "Batch 253/310 ━━━━━━━━━━━━━━━━━━━━ 03:21:56\n",
      "Accuracy: 0.4580 - Loss: 1.4511\n",
      "\n",
      "Batch 254/310 ━━━━━━━━━━━━━━━━━━━━ 03:22:44\n",
      "Accuracy: 0.4583 - Loss: 1.4497\n",
      "\n",
      "Batch 255/310 ━━━━━━━━━━━━━━━━━━━━ 03:23:32\n",
      "Accuracy: 0.4576 - Loss: 1.4498\n",
      "\n",
      "Batch 256/310 ━━━━━━━━━━━━━━━━━━━━ 03:24:20\n",
      "Accuracy: 0.4574 - Loss: 1.4494\n",
      "\n",
      "Batch 257/310 ━━━━━━━━━━━━━━━━━━━━ 03:25:07\n",
      "Accuracy: 0.4572 - Loss: 1.4495\n",
      "\n",
      "Batch 258/310 ━━━━━━━━━━━━━━━━━━━━ 03:25:56\n",
      "Accuracy: 0.4576 - Loss: 1.4488\n",
      "\n",
      "Batch 259/310 ━━━━━━━━━━━━━━━━━━━━ 03:26:44\n",
      "Accuracy: 0.4581 - Loss: 1.4476\n",
      "\n",
      "Batch 260/310 ━━━━━━━━━━━━━━━━━━━━ 03:27:32\n",
      "Accuracy: 0.4581 - Loss: 1.4477\n",
      "\n",
      "Batch 261/310 ━━━━━━━━━━━━━━━━━━━━ 03:28:19\n",
      "Accuracy: 0.4582 - Loss: 1.4473\n",
      "\n",
      "Batch 262/310 ━━━━━━━━━━━━━━━━━━━━ 03:29:07\n",
      "Accuracy: 0.4589 - Loss: 1.4460\n",
      "\n",
      "Batch 263/310 ━━━━━━━━━━━━━━━━━━━━ 03:29:55\n",
      "Accuracy: 0.4591 - Loss: 1.4452\n",
      "\n",
      "Batch 264/310 ━━━━━━━━━━━━━━━━━━━━ 03:30:44\n",
      "Accuracy: 0.4589 - Loss: 1.4452\n",
      "\n",
      "Batch 265/310 ━━━━━━━━━━━━━━━━━━━━ 03:31:32\n",
      "Accuracy: 0.4588 - Loss: 1.4448\n",
      "\n",
      "Batch 266/310 ━━━━━━━━━━━━━━━━━━━━ 03:32:19\n",
      "Accuracy: 0.4594 - Loss: 1.4438\n",
      "\n",
      "Batch 267/310 ━━━━━━━━━━━━━━━━━━━━ 03:33:07\n",
      "Accuracy: 0.4595 - Loss: 1.4433\n",
      "\n",
      "Batch 268/310 ━━━━━━━━━━━━━━━━━━━━ 03:33:56\n",
      "Accuracy: 0.4598 - Loss: 1.4427\n",
      "\n",
      "Batch 269/310 ━━━━━━━━━━━━━━━━━━━━ 03:34:43\n",
      "Accuracy: 0.4593 - Loss: 1.4429\n",
      "\n",
      "Batch 270/310 ━━━━━━━━━━━━━━━━━━━━ 03:35:31\n",
      "Accuracy: 0.4591 - Loss: 1.4432\n",
      "\n",
      "Batch 271/310 ━━━━━━━━━━━━━━━━━━━━ 03:36:19\n",
      "Accuracy: 0.4594 - Loss: 1.4422\n",
      "\n",
      "Batch 272/310 ━━━━━━━━━━━━━━━━━━━━ 03:37:07\n",
      "Accuracy: 0.4597 - Loss: 1.4416\n",
      "\n",
      "Batch 273/310 ━━━━━━━━━━━━━━━━━━━━ 03:37:55\n",
      "Accuracy: 0.4599 - Loss: 1.4412\n",
      "\n",
      "Batch 274/310 ━━━━━━━━━━━━━━━━━━━━ 03:38:43\n",
      "Accuracy: 0.4602 - Loss: 1.4407\n",
      "\n",
      "Batch 275/310 ━━━━━━━━━━━━━━━━━━━━ 03:39:31\n",
      "Accuracy: 0.4600 - Loss: 1.4406\n",
      "\n",
      "Batch 276/310 ━━━━━━━━━━━━━━━━━━━━ 03:40:19\n",
      "Accuracy: 0.4605 - Loss: 1.4395\n",
      "\n",
      "Batch 277/310 ━━━━━━━━━━━━━━━━━━━━ 03:41:06\n",
      "Accuracy: 0.4613 - Loss: 1.4384\n",
      "\n",
      "Batch 278/310 ━━━━━━━━━━━━━━━━━━━━ 03:41:55\n",
      "Accuracy: 0.4616 - Loss: 1.4374\n",
      "\n",
      "Batch 279/310 ━━━━━━━━━━━━━━━━━━━━ 03:42:42\n",
      "Accuracy: 0.4619 - Loss: 1.4365\n",
      "\n",
      "Batch 280/310 ━━━━━━━━━━━━━━━━━━━━ 03:43:30\n",
      "Accuracy: 0.4622 - Loss: 1.4358\n",
      "\n",
      "Batch 281/310 ━━━━━━━━━━━━━━━━━━━━ 03:44:18\n",
      "Accuracy: 0.4621 - Loss: 1.4358\n",
      "\n",
      "Batch 282/310 ━━━━━━━━━━━━━━━━━━━━ 03:45:06\n",
      "Accuracy: 0.4627 - Loss: 1.4346\n",
      "\n",
      "Batch 283/310 ━━━━━━━━━━━━━━━━━━━━ 03:45:54\n",
      "Accuracy: 0.4630 - Loss: 1.4341\n",
      "\n",
      "Batch 284/310 ━━━━━━━━━━━━━━━━━━━━ 03:46:41\n",
      "Accuracy: 0.4636 - Loss: 1.4335\n",
      "\n",
      "Batch 285/310 ━━━━━━━━━━━━━━━━━━━━ 03:47:29\n",
      "Accuracy: 0.4640 - Loss: 1.4323\n",
      "\n",
      "Batch 286/310 ━━━━━━━━━━━━━━━━━━━━ 03:48:18\n",
      "Accuracy: 0.4646 - Loss: 1.4314\n",
      "\n",
      "Batch 287/310 ━━━━━━━━━━━━━━━━━━━━ 03:49:05\n",
      "Accuracy: 0.4642 - Loss: 1.4316\n",
      "\n",
      "Batch 288/310 ━━━━━━━━━━━━━━━━━━━━ 03:49:53\n",
      "Accuracy: 0.4646 - Loss: 1.4309\n",
      "\n",
      "Batch 289/310 ━━━━━━━━━━━━━━━━━━━━ 03:50:41\n",
      "Accuracy: 0.4644 - Loss: 1.4310\n",
      "\n",
      "Batch 290/310 ━━━━━━━━━━━━━━━━━━━━ 03:51:29\n",
      "Accuracy: 0.4641 - Loss: 1.4318\n",
      "\n",
      "Batch 291/310 ━━━━━━━━━━━━━━━━━━━━ 03:52:17\n",
      "Accuracy: 0.4647 - Loss: 1.4306\n",
      "\n",
      "Batch 292/310 ━━━━━━━━━━━━━━━━━━━━ 03:53:05\n",
      "Accuracy: 0.4646 - Loss: 1.4304\n",
      "\n",
      "Batch 293/310 ━━━━━━━━━━━━━━━━━━━━ 03:53:53\n",
      "Accuracy: 0.4647 - Loss: 1.4301\n",
      "\n",
      "Batch 294/310 ━━━━━━━━━━━━━━━━━━━━ 03:54:41\n",
      "Accuracy: 0.4652 - Loss: 1.4289\n",
      "\n",
      "Batch 295/310 ━━━━━━━━━━━━━━━━━━━━ 03:55:29\n",
      "Accuracy: 0.4650 - Loss: 1.4284\n",
      "\n",
      "Batch 296/310 ━━━━━━━━━━━━━━━━━━━━ 03:56:18\n",
      "Accuracy: 0.4656 - Loss: 1.4275\n",
      "\n",
      "Batch 297/310 ━━━━━━━━━━━━━━━━━━━━ 03:57:06\n",
      "Accuracy: 0.4654 - Loss: 1.4271\n",
      "\n",
      "Batch 298/310 ━━━━━━━━━━━━━━━━━━━━ 03:57:54\n",
      "Accuracy: 0.4653 - Loss: 1.4274\n",
      "\n",
      "Batch 299/310 ━━━━━━━━━━━━━━━━━━━━ 03:58:42\n",
      "Accuracy: 0.4654 - Loss: 1.4266\n",
      "\n",
      "Batch 300/310 ━━━━━━━━━━━━━━━━━━━━ 03:59:29\n",
      "Accuracy: 0.4655 - Loss: 1.4264\n",
      "\n",
      "Batch 301/310 ━━━━━━━━━━━━━━━━━━━━ 04:00:18\n",
      "Accuracy: 0.4662 - Loss: 1.4254\n",
      "\n",
      "Batch 302/310 ━━━━━━━━━━━━━━━━━━━━ 04:01:06\n",
      "Accuracy: 0.4663 - Loss: 1.4250\n",
      "\n",
      "Batch 303/310 ━━━━━━━━━━━━━━━━━━━━ 04:01:54\n",
      "Accuracy: 0.4663 - Loss: 1.4248\n",
      "\n",
      "Batch 304/310 ━━━━━━━━━━━━━━━━━━━━ 04:02:42\n",
      "Accuracy: 0.4661 - Loss: 1.4245\n",
      "\n",
      "Batch 305/310 ━━━━━━━━━━━━━━━━━━━━ 04:03:29\n",
      "Accuracy: 0.4662 - Loss: 1.4241\n",
      "\n",
      "Batch 306/310 ━━━━━━━━━━━━━━━━━━━━ 04:04:19\n",
      "Accuracy: 0.4660 - Loss: 1.4247\n",
      "\n",
      "Batch 307/310 ━━━━━━━━━━━━━━━━━━━━ 04:05:06\n",
      "Accuracy: 0.4654 - Loss: 1.4252\n",
      "\n",
      "Batch 308/310 ━━━━━━━━━━━━━━━━━━━━ 04:05:54\n",
      "Accuracy: 0.4654 - Loss: 1.4250\n",
      "\n",
      "Batch 309/310 ━━━━━━━━━━━━━━━━━━━━ 04:06:41\n",
      "Accuracy: 0.4654 - Loss: 1.4249\n",
      "\n",
      "Batch 310/310 ━━━━━━━━━━━━━━━━━━━━ 04:07:29\n",
      "Accuracy: 0.4651 - Loss: 1.4250\n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 1/310 ━━━━━━━━━━━━━━━━━━━━ 05:10:11\n",
      "Accuracy: 0.5312 - Loss: 1.2791\n",
      "\n",
      "Batch 2/310 ━━━━━━━━━━━━━━━━━━━━ 05:11:00\n",
      "Accuracy: 0.5312 - Loss: 1.2098\n",
      "\n",
      "Batch 3/310 ━━━━━━━━━━━━━━━━━━━━ 05:11:47\n",
      "Accuracy: 0.5729 - Loss: 1.1795\n",
      "\n",
      "Batch 4/310 ━━━━━━━━━━━━━━━━━━━━ 05:12:36\n",
      "Accuracy: 0.5781 - Loss: 1.2031\n",
      "\n",
      "Batch 5/310 ━━━━━━━━━━━━━━━━━━━━ 05:13:24\n",
      "Accuracy: 0.5312 - Loss: 1.2956\n",
      "\n",
      "Batch 6/310 ━━━━━━━━━━━━━━━━━━━━ 05:14:12\n",
      "Accuracy: 0.5469 - Loss: 1.2368\n",
      "\n",
      "Batch 7/310 ━━━━━━━━━━━━━━━━━━━━ 05:15:00\n",
      "Accuracy: 0.5312 - Loss: 1.2444\n",
      "\n",
      "Batch 8/310 ━━━━━━━━━━━━━━━━━━━━ 05:15:47\n",
      "Accuracy: 0.5195 - Loss: 1.2649\n",
      "\n",
      "Batch 9/310 ━━━━━━━━━━━━━━━━━━━━ 05:16:36\n",
      "Accuracy: 0.5139 - Loss: 1.2649\n",
      "\n",
      "Batch 10/310 ━━━━━━━━━━━━━━━━━━━━ 05:17:24\n",
      "Accuracy: 0.5250 - Loss: 1.2461\n",
      "\n",
      "Batch 11/310 ━━━━━━━━━━━━━━━━━━━━ 05:18:12\n",
      "Accuracy: 0.5369 - Loss: 1.2317\n",
      "\n",
      "Batch 12/310 ━━━━━━━━━━━━━━━━━━━━ 05:19:00\n",
      "Accuracy: 0.5391 - Loss: 1.2219\n",
      "\n",
      "Batch 13/310 ━━━━━━━━━━━━━━━━━━━━ 05:19:48\n",
      "Accuracy: 0.5361 - Loss: 1.2268\n",
      "\n",
      "Batch 14/310 ━━━━━━━━━━━━━━━━━━━━ 05:20:36\n",
      "Accuracy: 0.5335 - Loss: 1.2258\n",
      "\n",
      "Batch 15/310 ━━━━━━━━━━━━━━━━━━━━ 05:21:24\n",
      "Accuracy: 0.5354 - Loss: 1.2286\n",
      "\n",
      "Batch 16/310 ━━━━━━━━━━━━━━━━━━━━ 05:22:12\n",
      "Accuracy: 0.5371 - Loss: 1.2334\n",
      "\n",
      "Batch 17/310 ━━━━━━━━━━━━━━━━━━━━ 05:23:00\n",
      "Accuracy: 0.5368 - Loss: 1.2392\n",
      "\n",
      "Batch 18/310 ━━━━━━━━━━━━━━━━━━━━ 05:23:48\n",
      "Accuracy: 0.5365 - Loss: 1.2317\n",
      "\n",
      "Batch 19/310 ━━━━━━━━━━━━━━━━━━━━ 05:24:36\n",
      "Accuracy: 0.5296 - Loss: 1.2399\n",
      "\n",
      "Batch 20/310 ━━━━━━━━━━━━━━━━━━━━ 05:25:24\n",
      "Accuracy: 0.5281 - Loss: 1.2441\n",
      "\n",
      "Batch 21/310 ━━━━━━━━━━━━━━━━━━━━ 05:26:12\n",
      "Accuracy: 0.5164 - Loss: 1.2658\n",
      "\n",
      "Batch 22/310 ━━━━━━━━━━━━━━━━━━━━ 05:27:00\n",
      "Accuracy: 0.5114 - Loss: 1.2714\n",
      "\n",
      "Batch 23/310 ━━━━━━━━━━━━━━━━━━━━ 05:27:47\n",
      "Accuracy: 0.5163 - Loss: 1.2699\n",
      "\n",
      "Batch 24/310 ━━━━━━━━━━━━━━━━━━━━ 05:28:35\n",
      "Accuracy: 0.5156 - Loss: 1.2715\n",
      "\n",
      "Batch 25/310 ━━━━━━━━━━━━━━━━━━━━ 05:29:24\n",
      "Accuracy: 0.5175 - Loss: 1.2731\n",
      "\n",
      "Batch 26/310 ━━━━━━━━━━━━━━━━━━━━ 05:30:12\n",
      "Accuracy: 0.5180 - Loss: 1.2706\n",
      "\n",
      "Batch 27/310 ━━━━━━━━━━━━━━━━━━━━ 05:31:00\n",
      "Accuracy: 0.5174 - Loss: 1.2773\n",
      "\n",
      "Batch 28/310 ━━━━━━━━━━━━━━━━━━━━ 05:31:47\n",
      "Accuracy: 0.5190 - Loss: 1.2724\n",
      "\n",
      "Batch 29/310 ━━━━━━━━━━━━━━━━━━━━ 05:32:35\n",
      "Accuracy: 0.5205 - Loss: 1.2752\n",
      "\n",
      "Batch 30/310 ━━━━━━━━━━━━━━━━━━━━ 05:33:23\n",
      "Accuracy: 0.5208 - Loss: 1.2762\n",
      "\n",
      "Batch 31/310 ━━━━━━━━━━━━━━━━━━━━ 05:34:11\n",
      "Accuracy: 0.5232 - Loss: 1.2701\n",
      "\n",
      "Batch 32/310 ━━━━━━━━━━━━━━━━━━━━ 05:34:59\n",
      "Accuracy: 0.5234 - Loss: 1.2715\n",
      "\n",
      "Batch 33/310 ━━━━━━━━━━━━━━━━━━━━ 05:35:47\n",
      "Accuracy: 0.5227 - Loss: 1.2740\n",
      "\n",
      "Batch 34/310 ━━━━━━━━━━━━━━━━━━━━ 05:36:35\n",
      "Accuracy: 0.5221 - Loss: 1.2723\n",
      "\n",
      "Batch 35/310 ━━━━━━━━━━━━━━━━━━━━ 05:37:22\n",
      "Accuracy: 0.5223 - Loss: 1.2718\n",
      "\n",
      "Batch 36/310 ━━━━━━━━━━━━━━━━━━━━ 05:38:10\n",
      "Accuracy: 0.5191 - Loss: 1.2747\n",
      "\n",
      "Batch 37/310 ━━━━━━━━━━━━━━━━━━━━ 05:38:59\n",
      "Accuracy: 0.5194 - Loss: 1.2739\n",
      "\n",
      "Batch 38/310 ━━━━━━━━━━━━━━━━━━━━ 05:39:47\n",
      "Accuracy: 0.5173 - Loss: 1.2784\n",
      "\n",
      "Batch 39/310 ━━━━━━━━━━━━━━━━━━━━ 05:40:35\n",
      "Accuracy: 0.5184 - Loss: 1.2734\n",
      "\n",
      "Batch 40/310 ━━━━━━━━━━━━━━━━━━━━ 05:41:23\n",
      "Accuracy: 0.5156 - Loss: 1.2758\n",
      "\n",
      "Batch 41/310 ━━━━━━━━━━━━━━━━━━━━ 05:42:11\n",
      "Accuracy: 0.5122 - Loss: 1.2797\n",
      "\n",
      "Batch 42/310 ━━━━━━━━━━━━━━━━━━━━ 05:42:59\n",
      "Accuracy: 0.5104 - Loss: 1.2853\n",
      "\n",
      "Batch 43/310 ━━━━━━━━━━━━━━━━━━━━ 05:43:47\n",
      "Accuracy: 0.5109 - Loss: 1.2873\n",
      "\n",
      "Batch 44/310 ━━━━━━━━━━━━━━━━━━━━ 05:44:36\n",
      "Accuracy: 0.5149 - Loss: 1.2837\n",
      "\n",
      "Batch 45/310 ━━━━━━━━━━━━━━━━━━━━ 05:45:23\n",
      "Accuracy: 0.5125 - Loss: 1.2885\n",
      "\n",
      "Batch 46/310 ━━━━━━━━━━━━━━━━━━━━ 05:46:11\n",
      "Accuracy: 0.5136 - Loss: 1.2880\n",
      "\n",
      "Batch 47/310 ━━━━━━━━━━━━━━━━━━━━ 05:47:00\n",
      "Accuracy: 0.5120 - Loss: 1.2879\n",
      "\n",
      "Batch 48/310 ━━━━━━━━━━━━━━━━━━━━ 05:47:48\n",
      "Accuracy: 0.5143 - Loss: 1.2823\n",
      "\n",
      "Batch 49/310 ━━━━━━━━━━━━━━━━━━━━ 05:48:36\n",
      "Accuracy: 0.5153 - Loss: 1.2814\n",
      "\n",
      "Batch 50/310 ━━━━━━━━━━━━━━━━━━━━ 05:49:23\n",
      "Accuracy: 0.5150 - Loss: 1.2850\n",
      "\n",
      "Batch 51/310 ━━━━━━━━━━━━━━━━━━━━ 05:50:11\n",
      "Accuracy: 0.5147 - Loss: 1.2868\n",
      "\n",
      "Batch 52/310 ━━━━━━━━━━━━━━━━━━━━ 05:51:00\n",
      "Accuracy: 0.5174 - Loss: 1.2834\n",
      "\n",
      "Batch 53/310 ━━━━━━━━━━━━━━━━━━━━ 05:51:48\n",
      "Accuracy: 0.5159 - Loss: 1.2834\n",
      "\n",
      "Batch 54/310 ━━━━━━━━━━━━━━━━━━━━ 05:52:36\n",
      "Accuracy: 0.5127 - Loss: 1.2890\n",
      "\n",
      "Batch 55/310 ━━━━━━━━━━━━━━━━━━━━ 05:53:23\n",
      "Accuracy: 0.5125 - Loss: 1.2895\n",
      "\n",
      "Batch 56/310 ━━━━━━━━━━━━━━━━━━━━ 05:54:11\n",
      "Accuracy: 0.5140 - Loss: 1.2874\n",
      "\n",
      "Batch 57/310 ━━━━━━━━━━━━━━━━━━━━ 05:55:00\n",
      "Accuracy: 0.5143 - Loss: 1.2888\n",
      "\n",
      "Batch 58/310 ━━━━━━━━━━━━━━━━━━━━ 05:55:48\n",
      "Accuracy: 0.5140 - Loss: 1.2887\n",
      "\n",
      "Batch 59/310 ━━━━━━━━━━━━━━━━━━━━ 05:56:36\n",
      "Accuracy: 0.5127 - Loss: 1.2920\n",
      "\n",
      "Batch 60/310 ━━━━━━━━━━━━━━━━━━━━ 05:57:24\n",
      "Accuracy: 0.5146 - Loss: 1.2886\n",
      "\n",
      "Batch 61/310 ━━━━━━━━━━━━━━━━━━━━ 05:58:11\n",
      "Accuracy: 0.5149 - Loss: 1.2890\n",
      "\n",
      "Batch 62/310 ━━━━━━━━━━━━━━━━━━━━ 05:59:00\n",
      "Accuracy: 0.5141 - Loss: 1.2896\n",
      "\n",
      "Batch 63/310 ━━━━━━━━━━━━━━━━━━━━ 05:59:48\n",
      "Accuracy: 0.5164 - Loss: 1.2868\n",
      "\n",
      "Batch 64/310 ━━━━━━━━━━━━━━━━━━━━ 06:00:36\n",
      "Accuracy: 0.5156 - Loss: 1.2874\n",
      "\n",
      "Batch 65/310 ━━━━━━━━━━━━━━━━━━━━ 06:01:24\n",
      "Accuracy: 0.5154 - Loss: 1.2871\n",
      "\n",
      "Batch 66/310 ━━━━━━━━━━━━━━━━━━━━ 06:02:12\n",
      "Accuracy: 0.5128 - Loss: 1.2911\n",
      "\n",
      "Batch 67/310 ━━━━━━━━━━━━━━━━━━━━ 06:03:01\n",
      "Accuracy: 0.5140 - Loss: 1.2899\n",
      "\n",
      "Batch 68/310 ━━━━━━━━━━━━━━━━━━━━ 06:03:49\n",
      "Accuracy: 0.5156 - Loss: 1.2895\n",
      "\n",
      "Batch 69/310 ━━━━━━━━━━━━━━━━━━━━ 06:04:37\n",
      "Accuracy: 0.5149 - Loss: 1.2905\n",
      "\n",
      "Batch 70/310 ━━━━━━━━━━━━━━━━━━━━ 06:05:25\n",
      "Accuracy: 0.5156 - Loss: 1.2902\n",
      "\n",
      "Batch 71/310 ━━━━━━━━━━━━━━━━━━━━ 06:06:13\n",
      "Accuracy: 0.5145 - Loss: 1.2917\n",
      "\n",
      "Batch 72/310 ━━━━━━━━━━━━━━━━━━━━ 06:07:02\n",
      "Accuracy: 0.5139 - Loss: 1.2924\n",
      "\n",
      "Batch 73/310 ━━━━━━━━━━━━━━━━━━━━ 06:07:49\n",
      "Accuracy: 0.5141 - Loss: 1.2919\n",
      "\n",
      "Batch 74/310 ━━━━━━━━━━━━━━━━━━━━ 06:08:38\n",
      "Accuracy: 0.5165 - Loss: 1.2908\n",
      "\n",
      "Batch 75/310 ━━━━━━━━━━━━━━━━━━━━ 06:09:25\n",
      "Accuracy: 0.5167 - Loss: 1.2906\n",
      "\n",
      "Batch 76/310 ━━━━━━━━━━━━━━━━━━━━ 06:10:13\n",
      "Accuracy: 0.5156 - Loss: 1.2902\n",
      "\n",
      "Batch 77/310 ━━━━━━━━━━━━━━━━━━━━ 06:11:03\n",
      "Accuracy: 0.5170 - Loss: 1.2890\n",
      "\n",
      "Batch 78/310 ━━━━━━━━━━━━━━━━━━━━ 06:11:50\n",
      "Accuracy: 0.5184 - Loss: 1.2864\n",
      "\n",
      "Batch 79/310 ━━━━━━━━━━━━━━━━━━━━ 06:12:38\n",
      "Accuracy: 0.5178 - Loss: 1.2877\n",
      "\n",
      "Batch 80/310 ━━━━━━━━━━━━━━━━━━━━ 06:13:26\n",
      "Accuracy: 0.5195 - Loss: 1.2858\n",
      "\n",
      "Batch 81/310 ━━━━━━━━━━━━━━━━━━━━ 06:14:14\n",
      "Accuracy: 0.5212 - Loss: 1.2837\n",
      "\n",
      "Batch 82/310 ━━━━━━━━━━━━━━━━━━━━ 06:15:03\n",
      "Accuracy: 0.5217 - Loss: 1.2827\n",
      "\n",
      "Batch 83/310 ━━━━━━━━━━━━━━━━━━━━ 06:15:51\n",
      "Accuracy: 0.5222 - Loss: 1.2816\n",
      "\n",
      "Batch 84/310 ━━━━━━━━━━━━━━━━━━━━ 06:16:39\n",
      "Accuracy: 0.5223 - Loss: 1.2822\n",
      "\n",
      "Batch 85/310 ━━━━━━━━━━━━━━━━━━━━ 06:17:27\n",
      "Accuracy: 0.5224 - Loss: 1.2827\n",
      "\n",
      "Batch 86/310 ━━━━━━━━━━━━━━━━━━━━ 06:18:14\n",
      "Accuracy: 0.5233 - Loss: 1.2819\n",
      "\n",
      "Batch 87/310 ━━━━━━━━━━━━━━━━━━━━ 06:19:03\n",
      "Accuracy: 0.5241 - Loss: 1.2814\n",
      "\n",
      "Batch 88/310 ━━━━━━━━━━━━━━━━━━━━ 06:19:50\n",
      "Accuracy: 0.5234 - Loss: 1.2823\n",
      "\n",
      "Batch 89/310 ━━━━━━━━━━━━━━━━━━━━ 06:20:39\n",
      "Accuracy: 0.5228 - Loss: 1.2823\n",
      "\n",
      "Batch 90/310 ━━━━━━━━━━━━━━━━━━━━ 06:21:26\n",
      "Accuracy: 0.5247 - Loss: 1.2801\n",
      "\n",
      "Batch 91/310 ━━━━━━━━━━━━━━━━━━━━ 06:22:14\n",
      "Accuracy: 0.5261 - Loss: 1.2789\n",
      "\n",
      "Batch 92/310 ━━━━━━━━━━━━━━━━━━━━ 06:23:02\n",
      "Accuracy: 0.5265 - Loss: 1.2787\n",
      "\n",
      "Batch 93/310 ━━━━━━━━━━━━━━━━━━━━ 06:23:50\n",
      "Accuracy: 0.5269 - Loss: 1.2789\n",
      "\n",
      "Batch 94/310 ━━━━━━━━━━━━━━━━━━━━ 06:24:38\n",
      "Accuracy: 0.5276 - Loss: 1.2782\n",
      "\n",
      "Batch 95/310 ━━━━━━━━━━━━━━━━━━━━ 06:25:26\n",
      "Accuracy: 0.5270 - Loss: 1.2781\n",
      "\n",
      "Batch 96/310 ━━━━━━━━━━━━━━━━━━━━ 06:26:13\n",
      "Accuracy: 0.5260 - Loss: 1.2788\n",
      "\n",
      "Batch 97/310 ━━━━━━━━━━━━━━━━━━━━ 06:27:02\n",
      "Accuracy: 0.5271 - Loss: 1.2774\n",
      "\n",
      "Batch 98/310 ━━━━━━━━━━━━━━━━━━━━ 06:27:49\n",
      "Accuracy: 0.5271 - Loss: 1.2783\n",
      "\n",
      "Batch 99/310 ━━━━━━━━━━━━━━━━━━━━ 06:28:37\n",
      "Accuracy: 0.5278 - Loss: 1.2797\n",
      "\n",
      "Batch 100/310 ━━━━━━━━━━━━━━━━━━━━ 06:29:26\n",
      "Accuracy: 0.5294 - Loss: 1.2784\n",
      "\n",
      "Batch 101/310 ━━━━━━━━━━━━━━━━━━━━ 06:30:13\n",
      "Accuracy: 0.5294 - Loss: 1.2785\n",
      "\n",
      "Batch 102/310 ━━━━━━━━━━━━━━━━━━━━ 06:31:02\n",
      "Accuracy: 0.5291 - Loss: 1.2797\n",
      "\n",
      "Batch 103/310 ━━━━━━━━━━━━━━━━━━━━ 06:31:49\n",
      "Accuracy: 0.5291 - Loss: 1.2789\n",
      "\n",
      "Batch 104/310 ━━━━━━━━━━━━━━━━━━━━ 06:32:38\n",
      "Accuracy: 0.5291 - Loss: 1.2792\n",
      "\n",
      "Batch 105/310 ━━━━━━━━━━━━━━━━━━━━ 06:33:26\n",
      "Accuracy: 0.5289 - Loss: 1.2786\n",
      "\n",
      "Batch 106/310 ━━━━━━━━━━━━━━━━━━━━ 06:34:14\n",
      "Accuracy: 0.5295 - Loss: 1.2778\n",
      "\n",
      "Batch 107/310 ━━━━━━━━━━━━━━━━━━━━ 06:35:02\n",
      "Accuracy: 0.5307 - Loss: 1.2764\n",
      "\n",
      "Batch 108/310 ━━━━━━━━━━━━━━━━━━━━ 06:35:49\n",
      "Accuracy: 0.5295 - Loss: 1.2780\n",
      "\n",
      "Batch 109/310 ━━━━━━━━━━━━━━━━━━━━ 06:36:37\n",
      "Accuracy: 0.5278 - Loss: 1.2807\n",
      "\n",
      "Batch 110/310 ━━━━━━━━━━━━━━━━━━━━ 06:37:25\n",
      "Accuracy: 0.5295 - Loss: 1.2790\n",
      "\n",
      "Batch 111/310 ━━━━━━━━━━━━━━━━━━━━ 06:38:13\n",
      "Accuracy: 0.5310 - Loss: 1.2767\n",
      "\n",
      "Batch 112/310 ━━━━━━━━━━━━━━━━━━━━ 06:39:01\n",
      "Accuracy: 0.5296 - Loss: 1.2787\n",
      "\n",
      "Batch 113/310 ━━━━━━━━━━━━━━━━━━━━ 06:39:48\n",
      "Accuracy: 0.5310 - Loss: 1.2782\n",
      "\n",
      "Batch 114/310 ━━━━━━━━━━━━━━━━━━━━ 06:40:36\n",
      "Accuracy: 0.5329 - Loss: 1.2754\n",
      "\n",
      "Batch 115/310 ━━━━━━━━━━━━━━━━━━━━ 06:41:24\n",
      "Accuracy: 0.5329 - Loss: 1.2746\n",
      "\n",
      "Batch 116/310 ━━━━━━━━━━━━━━━━━━━━ 06:42:12\n",
      "Accuracy: 0.5331 - Loss: 1.2741\n",
      "\n",
      "Batch 117/310 ━━━━━━━━━━━━━━━━━━━━ 06:43:00\n",
      "Accuracy: 0.5321 - Loss: 1.2763\n",
      "\n",
      "Batch 118/310 ━━━━━━━━━━━━━━━━━━━━ 06:43:48\n",
      "Accuracy: 0.5328 - Loss: 1.2760\n",
      "\n",
      "Batch 119/310 ━━━━━━━━━━━━━━━━━━━━ 06:44:36\n",
      "Accuracy: 0.5331 - Loss: 1.2750\n",
      "\n",
      "Batch 120/310 ━━━━━━━━━━━━━━━━━━━━ 06:45:25\n",
      "Accuracy: 0.5331 - Loss: 1.2751\n",
      "\n",
      "Batch 121/310 ━━━━━━━━━━━━━━━━━━━━ 06:46:12\n",
      "Accuracy: 0.5338 - Loss: 1.2730\n",
      "\n",
      "Batch 122/310 ━━━━━━━━━━━━━━━━━━━━ 06:47:00\n",
      "Accuracy: 0.5343 - Loss: 1.2730\n",
      "\n",
      "Batch 123/310 ━━━━━━━━━━━━━━━━━━━━ 06:47:48\n",
      "Accuracy: 0.5335 - Loss: 1.2744\n",
      "\n",
      "Batch 124/310 ━━━━━━━━━━━━━━━━━━━━ 06:48:36\n",
      "Accuracy: 0.5340 - Loss: 1.2729\n",
      "\n",
      "Batch 125/310 ━━━━━━━━━━━━━━━━━━━━ 06:49:24\n",
      "Accuracy: 0.5350 - Loss: 1.2711\n",
      "\n",
      "Batch 126/310 ━━━━━━━━━━━━━━━━━━━━ 06:50:12\n",
      "Accuracy: 0.5357 - Loss: 1.2703\n",
      "\n",
      "Batch 127/310 ━━━━━━━━━━━━━━━━━━━━ 06:51:00\n",
      "Accuracy: 0.5359 - Loss: 1.2692\n",
      "\n",
      "Batch 128/310 ━━━━━━━━━━━━━━━━━━━━ 06:51:48\n",
      "Accuracy: 0.5359 - Loss: 1.2695\n",
      "\n",
      "Batch 129/310 ━━━━━━━━━━━━━━━━━━━━ 06:52:36\n",
      "Accuracy: 0.5366 - Loss: 1.2682\n",
      "\n",
      "Batch 130/310 ━━━━━━━━━━━━━━━━━━━━ 06:53:25\n",
      "Accuracy: 0.5377 - Loss: 1.2672\n",
      "\n",
      "Batch 131/310 ━━━━━━━━━━━━━━━━━━━━ 06:54:13\n",
      "Accuracy: 0.5396 - Loss: 1.2652\n",
      "\n",
      "Batch 132/310 ━━━━━━━━━━━━━━━━━━━━ 06:55:01\n",
      "Accuracy: 0.5407 - Loss: 1.2638\n",
      "\n",
      "Batch 133/310 ━━━━━━━━━━━━━━━━━━━━ 06:55:49\n",
      "Accuracy: 0.5404 - Loss: 1.2633\n",
      "\n",
      "Batch 134/310 ━━━━━━━━━━━━━━━━━━━━ 06:56:36\n",
      "Accuracy: 0.5403 - Loss: 1.2635\n",
      "\n",
      "Batch 135/310 ━━━━━━━━━━━━━━━━━━━━ 06:57:25\n",
      "Accuracy: 0.5417 - Loss: 1.2608\n",
      "\n",
      "Batch 136/310 ━━━━━━━━━━━━━━━━━━━━ 06:58:13\n",
      "Accuracy: 0.5409 - Loss: 1.2621\n",
      "\n",
      "Batch 137/310 ━━━━━━━━━━━━━━━━━━━━ 06:59:01\n",
      "Accuracy: 0.5411 - Loss: 1.2622\n",
      "\n",
      "Batch 138/310 ━━━━━━━━━━━━━━━━━━━━ 06:59:49\n",
      "Accuracy: 0.5401 - Loss: 1.2641\n",
      "\n",
      "Batch 139/310 ━━━━━━━━━━━━━━━━━━━━ 07:00:36\n",
      "Accuracy: 0.5396 - Loss: 1.2641\n",
      "\n",
      "Batch 140/310 ━━━━━━━━━━━━━━━━━━━━ 07:01:25\n",
      "Accuracy: 0.5384 - Loss: 1.2654\n",
      "\n",
      "Batch 141/310 ━━━━━━━━━━━━━━━━━━━━ 07:02:13\n",
      "Accuracy: 0.5377 - Loss: 1.2674\n",
      "\n",
      "Batch 142/310 ━━━━━━━━━━━━━━━━━━━━ 07:03:01\n",
      "Accuracy: 0.5379 - Loss: 1.2671\n",
      "\n",
      "Batch 143/310 ━━━━━━━━━━━━━━━━━━━━ 07:03:49\n",
      "Accuracy: 0.5391 - Loss: 1.2645\n",
      "\n",
      "Batch 144/310 ━━━━━━━━━━━━━━━━━━━━ 07:04:36\n",
      "Accuracy: 0.5393 - Loss: 1.2642\n",
      "\n",
      "Batch 145/310 ━━━━━━━━━━━━━━━━━━━━ 07:05:25\n",
      "Accuracy: 0.5390 - Loss: 1.2651\n",
      "\n",
      "Batch 146/310 ━━━━━━━━━━━━━━━━━━━━ 07:06:13\n",
      "Accuracy: 0.5390 - Loss: 1.2652\n",
      "\n",
      "Batch 147/310 ━━━━━━━━━━━━━━━━━━━━ 07:07:01\n",
      "Accuracy: 0.5391 - Loss: 1.2652\n",
      "\n",
      "Batch 148/310 ━━━━━━━━━━━━━━━━━━━━ 07:07:49\n",
      "Accuracy: 0.5393 - Loss: 1.2652\n",
      "\n",
      "Batch 149/310 ━━━━━━━━━━━━━━━━━━━━ 07:08:36\n",
      "Accuracy: 0.5394 - Loss: 1.2645\n",
      "\n",
      "Batch 150/310 ━━━━━━━━━━━━━━━━━━━━ 07:09:25\n",
      "Accuracy: 0.5400 - Loss: 1.2626\n",
      "\n",
      "Batch 151/310 ━━━━━━━━━━━━━━━━━━━━ 07:10:13\n",
      "Accuracy: 0.5401 - Loss: 1.2618\n",
      "\n",
      "Batch 152/310 ━━━━━━━━━━━━━━━━━━━━ 07:11:01\n",
      "Accuracy: 0.5403 - Loss: 1.2603\n",
      "\n",
      "Batch 153/310 ━━━━━━━━━━━━━━━━━━━━ 07:11:49\n",
      "Accuracy: 0.5413 - Loss: 1.2586\n",
      "\n",
      "Batch 154/310 ━━━━━━━━━━━━━━━━━━━━ 07:12:36\n",
      "Accuracy: 0.5410 - Loss: 1.2596\n",
      "\n",
      "Batch 155/310 ━━━━━━━━━━━━━━━━━━━━ 07:13:26\n",
      "Accuracy: 0.5413 - Loss: 1.2584\n",
      "\n",
      "Batch 156/310 ━━━━━━━━━━━━━━━━━━━━ 07:14:14\n",
      "Accuracy: 0.5417 - Loss: 1.2576\n",
      "\n",
      "Batch 157/310 ━━━━━━━━━━━━━━━━━━━━ 07:15:02\n",
      "Accuracy: 0.5416 - Loss: 1.2565\n",
      "\n",
      "Batch 158/310 ━━━━━━━━━━━━━━━━━━━━ 07:15:50\n",
      "Accuracy: 0.5421 - Loss: 1.2552\n",
      "\n",
      "Batch 159/310 ━━━━━━━━━━━━━━━━━━━━ 07:16:37\n",
      "Accuracy: 0.5421 - Loss: 1.2548\n",
      "\n",
      "Batch 160/310 ━━━━━━━━━━━━━━━━━━━━ 07:17:26\n",
      "Accuracy: 0.5418 - Loss: 1.2544\n",
      "\n",
      "Batch 161/310 ━━━━━━━━━━━━━━━━━━━━ 07:18:15\n",
      "Accuracy: 0.5411 - Loss: 1.2562\n",
      "\n",
      "Batch 162/310 ━━━━━━━━━━━━━━━━━━━━ 07:19:03\n",
      "Accuracy: 0.5417 - Loss: 1.2552\n",
      "\n",
      "Batch 163/310 ━━━━━━━━━━━━━━━━━━━━ 07:19:51\n",
      "Accuracy: 0.5412 - Loss: 1.2558\n",
      "\n",
      "Batch 164/310 ━━━━━━━━━━━━━━━━━━━━ 07:20:38\n",
      "Accuracy: 0.5406 - Loss: 1.2566\n",
      "\n",
      "Batch 165/310 ━━━━━━━━━━━━━━━━━━━━ 07:21:27\n",
      "Accuracy: 0.5411 - Loss: 1.2564\n",
      "\n",
      "Batch 166/310 ━━━━━━━━━━━━━━━━━━━━ 07:22:15\n",
      "Accuracy: 0.5414 - Loss: 1.2564\n",
      "\n",
      "Batch 167/310 ━━━━━━━━━━━━━━━━━━━━ 07:23:03\n",
      "Accuracy: 0.5417 - Loss: 1.2555\n",
      "\n",
      "Batch 168/310 ━━━━━━━━━━━━━━━━━━━━ 07:23:50\n",
      "Accuracy: 0.5422 - Loss: 1.2545\n",
      "\n",
      "Batch 169/310 ━━━━━━━━━━━━━━━━━━━━ 07:24:38\n",
      "Accuracy: 0.5433 - Loss: 1.2532\n",
      "\n",
      "Batch 170/310 ━━━━━━━━━━━━━━━━━━━━ 07:25:27\n",
      "Accuracy: 0.5436 - Loss: 1.2524\n",
      "\n",
      "Batch 171/310 ━━━━━━━━━━━━━━━━━━━━ 07:26:14\n",
      "Accuracy: 0.5440 - Loss: 1.2519\n",
      "\n",
      "Batch 172/310 ━━━━━━━━━━━━━━━━━━━━ 07:27:02\n",
      "Accuracy: 0.5440 - Loss: 1.2515\n",
      "\n",
      "Batch 173/310 ━━━━━━━━━━━━━━━━━━━━ 07:27:50\n",
      "Accuracy: 0.5450 - Loss: 1.2502\n",
      "\n",
      "Batch 174/310 ━━━━━━━━━━━━━━━━━━━━ 07:28:37\n",
      "Accuracy: 0.5460 - Loss: 1.2489\n",
      "\n",
      "Batch 175/310 ━━━━━━━━━━━━━━━━━━━━ 07:29:26\n",
      "Accuracy: 0.5457 - Loss: 1.2484\n",
      "\n",
      "Batch 176/310 ━━━━━━━━━━━━━━━━━━━━ 07:30:14\n",
      "Accuracy: 0.5460 - Loss: 1.2482\n",
      "\n",
      "Batch 177/310 ━━━━━━━━━━━━━━━━━━━━ 07:31:02\n",
      "Accuracy: 0.5464 - Loss: 1.2475\n",
      "\n",
      "Batch 178/310 ━━━━━━━━━━━━━━━━━━━━ 07:31:49\n",
      "Accuracy: 0.5462 - Loss: 1.2474\n",
      "\n",
      "Batch 179/310 ━━━━━━━━━━━━━━━━━━━━ 07:32:37\n",
      "Accuracy: 0.5463 - Loss: 1.2472\n",
      "\n",
      "Batch 180/310 ━━━━━━━━━━━━━━━━━━━━ 07:33:25\n",
      "Accuracy: 0.5460 - Loss: 1.2466\n",
      "\n",
      "Batch 181/310 ━━━━━━━━━━━━━━━━━━━━ 07:34:13\n",
      "Accuracy: 0.5463 - Loss: 1.2464\n",
      "\n",
      "Batch 182/310 ━━━━━━━━━━━━━━━━━━━━ 07:35:01\n",
      "Accuracy: 0.5462 - Loss: 1.2461\n",
      "\n",
      "Batch 183/310 ━━━━━━━━━━━━━━━━━━━━ 07:35:48\n",
      "Accuracy: 0.5470 - Loss: 1.2449\n",
      "\n",
      "Batch 184/310 ━━━━━━━━━━━━━━━━━━━━ 07:36:36\n",
      "Accuracy: 0.5479 - Loss: 1.2434\n",
      "\n",
      "Batch 185/310 ━━━━━━━━━━━━━━━━━━━━ 07:37:24\n",
      "Accuracy: 0.5476 - Loss: 1.2441\n",
      "\n",
      "Batch 186/310 ━━━━━━━━━━━━━━━━━━━━ 07:38:11\n",
      "Accuracy: 0.5472 - Loss: 1.2445\n",
      "\n",
      "Batch 187/310 ━━━━━━━━━━━━━━━━━━━━ 07:39:00\n",
      "Accuracy: 0.5470 - Loss: 1.2438\n",
      "\n",
      "Batch 188/310 ━━━━━━━━━━━━━━━━━━━━ 07:39:48\n",
      "Accuracy: 0.5472 - Loss: 1.2437\n",
      "\n",
      "Batch 189/310 ━━━━━━━━━━━━━━━━━━━━ 07:40:35\n",
      "Accuracy: 0.5481 - Loss: 1.2422\n",
      "\n",
      "Batch 190/310 ━━━━━━━━━━━━━━━━━━━━ 07:41:24\n",
      "Accuracy: 0.5480 - Loss: 1.2419\n",
      "\n",
      "Batch 191/310 ━━━━━━━━━━━━━━━━━━━━ 07:42:11\n",
      "Accuracy: 0.5481 - Loss: 1.2415\n",
      "\n",
      "Batch 192/310 ━━━━━━━━━━━━━━━━━━━━ 07:42:59\n",
      "Accuracy: 0.5485 - Loss: 1.2411\n",
      "\n",
      "Batch 193/310 ━━━━━━━━━━━━━━━━━━━━ 07:43:47\n",
      "Accuracy: 0.5487 - Loss: 1.2402\n",
      "\n",
      "Batch 194/310 ━━━━━━━━━━━━━━━━━━━━ 07:44:35\n",
      "Accuracy: 0.5486 - Loss: 1.2401\n",
      "\n",
      "Batch 195/310 ━━━━━━━━━━━━━━━━━━━━ 07:45:23\n",
      "Accuracy: 0.5490 - Loss: 1.2395\n",
      "\n",
      "Batch 196/310 ━━━━━━━━━━━━━━━━━━━━ 07:46:11\n",
      "Accuracy: 0.5493 - Loss: 1.2388\n",
      "\n",
      "Batch 197/310 ━━━━━━━━━━━━━━━━━━━━ 07:46:58\n",
      "Accuracy: 0.5493 - Loss: 1.2385\n",
      "\n",
      "Batch 198/310 ━━━━━━━━━━━━━━━━━━━━ 07:47:47\n",
      "Accuracy: 0.5489 - Loss: 1.2384\n",
      "\n",
      "Batch 199/310 ━━━━━━━━━━━━━━━━━━━━ 07:48:35\n",
      "Accuracy: 0.5493 - Loss: 1.2382\n",
      "\n",
      "Batch 200/310 ━━━━━━━━━━━━━━━━━━━━ 07:49:23\n",
      "Accuracy: 0.5498 - Loss: 1.2376\n",
      "\n",
      "Batch 201/310 ━━━━━━━━━━━━━━━━━━━━ 07:50:11\n",
      "Accuracy: 0.5499 - Loss: 1.2368\n",
      "\n",
      "Batch 202/310 ━━━━━━━━━━━━━━━━━━━━ 07:50:58\n",
      "Accuracy: 0.5492 - Loss: 1.2382\n",
      "\n",
      "Batch 203/310 ━━━━━━━━━━━━━━━━━━━━ 07:51:47\n",
      "Accuracy: 0.5491 - Loss: 1.2386\n",
      "\n",
      "Batch 204/310 ━━━━━━━━━━━━━━━━━━━━ 07:52:35\n",
      "Accuracy: 0.5496 - Loss: 1.2377\n",
      "\n",
      "Batch 205/310 ━━━━━━━━━━━━━━━━━━━━ 07:53:23\n",
      "Accuracy: 0.5503 - Loss: 1.2375\n",
      "\n",
      "Batch 206/310 ━━━━━━━━━━━━━━━━━━━━ 07:54:11\n",
      "Accuracy: 0.5510 - Loss: 1.2364\n",
      "\n",
      "Batch 207/310 ━━━━━━━━━━━━━━━━━━━━ 07:54:58\n",
      "Accuracy: 0.5515 - Loss: 1.2353\n",
      "\n",
      "Batch 208/310 ━━━━━━━━━━━━━━━━━━━━ 07:55:48\n",
      "Accuracy: 0.5518 - Loss: 1.2350\n",
      "\n",
      "Batch 209/310 ━━━━━━━━━━━━━━━━━━━━ 07:56:35\n",
      "Accuracy: 0.5522 - Loss: 1.2341\n",
      "\n",
      "Batch 210/310 ━━━━━━━━━━━━━━━━━━━━ 07:57:23\n",
      "Accuracy: 0.5518 - Loss: 1.2348\n",
      "\n",
      "Batch 211/310 ━━━━━━━━━━━━━━━━━━━━ 07:58:11\n",
      "Accuracy: 0.5515 - Loss: 1.2348\n",
      "\n",
      "Batch 212/310 ━━━━━━━━━━━━━━━━━━━━ 07:58:59\n",
      "Accuracy: 0.5509 - Loss: 1.2359\n",
      "\n",
      "Batch 213/310 ━━━━━━━━━━━━━━━━━━━━ 07:59:48\n",
      "Accuracy: 0.5512 - Loss: 1.2349\n",
      "\n",
      "Batch 214/310 ━━━━━━━━━━━━━━━━━━━━ 08:00:36\n",
      "Accuracy: 0.5510 - Loss: 1.2358\n",
      "\n",
      "Batch 215/310 ━━━━━━━━━━━━━━━━━━━━ 08:01:24\n",
      "Accuracy: 0.5503 - Loss: 1.2363\n",
      "\n",
      "Batch 216/310 ━━━━━━━━━━━━━━━━━━━━ 08:02:12\n",
      "Accuracy: 0.5508 - Loss: 1.2356\n",
      "\n",
      "Batch 217/310 ━━━━━━━━━━━━━━━━━━━━ 08:03:00\n",
      "Accuracy: 0.5511 - Loss: 1.2355\n",
      "\n",
      "Batch 218/310 ━━━━━━━━━━━━━━━━━━━━ 08:03:49\n",
      "Accuracy: 0.5507 - Loss: 1.2361\n",
      "\n",
      "Batch 219/310 ━━━━━━━━━━━━━━━━━━━━ 08:04:37\n",
      "Accuracy: 0.5508 - Loss: 1.2358\n",
      "\n",
      "Batch 220/310 ━━━━━━━━━━━━━━━━━━━━ 08:05:25\n",
      "Accuracy: 0.5520 - Loss: 1.2342\n",
      "\n",
      "Batch 221/310 ━━━━━━━━━━━━━━━━━━━━ 08:06:12\n",
      "Accuracy: 0.5526 - Loss: 1.2331\n",
      "\n",
      "Batch 222/310 ━━━━━━━━━━━━━━━━━━━━ 08:07:00\n",
      "Accuracy: 0.5528 - Loss: 1.2332\n",
      "\n",
      "Batch 223/310 ━━━━━━━━━━━━━━━━━━━━ 08:07:49\n",
      "Accuracy: 0.5527 - Loss: 1.2332\n",
      "\n",
      "Batch 224/310 ━━━━━━━━━━━━━━━━━━━━ 08:08:37\n",
      "Accuracy: 0.5536 - Loss: 1.2319\n",
      "\n",
      "Batch 225/310 ━━━━━━━━━━━━━━━━━━━━ 08:09:25\n",
      "Accuracy: 0.5537 - Loss: 1.2309\n",
      "\n",
      "Batch 226/310 ━━━━━━━━━━━━━━━━━━━━ 08:10:13\n",
      "Accuracy: 0.5535 - Loss: 1.2316\n",
      "\n",
      "Batch 227/310 ━━━━━━━━━━━━━━━━━━━━ 08:11:00\n",
      "Accuracy: 0.5537 - Loss: 1.2307\n",
      "\n",
      "Batch 228/310 ━━━━━━━━━━━━━━━━━━━━ 08:11:49\n",
      "Accuracy: 0.5537 - Loss: 1.2304\n",
      "\n",
      "Batch 229/310 ━━━━━━━━━━━━━━━━━━━━ 08:12:37\n",
      "Accuracy: 0.5540 - Loss: 1.2294\n",
      "\n",
      "Batch 230/310 ━━━━━━━━━━━━━━━━━━━━ 08:13:25\n",
      "Accuracy: 0.5534 - Loss: 1.2297\n",
      "\n",
      "Batch 231/310 ━━━━━━━━━━━━━━━━━━━━ 08:14:13\n",
      "Accuracy: 0.5533 - Loss: 1.2297\n",
      "\n",
      "Batch 232/310 ━━━━━━━━━━━━━━━━━━━━ 08:15:01\n",
      "Accuracy: 0.5535 - Loss: 1.2295\n",
      "\n",
      "Batch 233/310 ━━━━━━━━━━━━━━━━━━━━ 08:15:50\n",
      "Accuracy: 0.5535 - Loss: 1.2293\n",
      "\n",
      "Batch 234/310 ━━━━━━━━━━━━━━━━━━━━ 08:16:37\n",
      "Accuracy: 0.5530 - Loss: 1.2299\n",
      "\n",
      "Batch 235/310 ━━━━━━━━━━━━━━━━━━━━ 08:17:26\n",
      "Accuracy: 0.5531 - Loss: 1.2291\n",
      "\n",
      "Batch 236/310 ━━━━━━━━━━━━━━━━━━━━ 08:18:13\n",
      "Accuracy: 0.5527 - Loss: 1.2302\n",
      "\n",
      "Batch 237/310 ━━━━━━━━━━━━━━━━━━━━ 08:19:01\n",
      "Accuracy: 0.5533 - Loss: 1.2297\n",
      "\n",
      "Batch 238/310 ━━━━━━━━━━━━━━━━━━━━ 08:19:50\n",
      "Accuracy: 0.5534 - Loss: 1.2296\n",
      "\n",
      "Batch 239/310 ━━━━━━━━━━━━━━━━━━━━ 08:20:37\n",
      "Accuracy: 0.5537 - Loss: 1.2290\n",
      "\n",
      "Batch 240/310 ━━━━━━━━━━━━━━━━━━━━ 08:21:25\n",
      "Accuracy: 0.5536 - Loss: 1.2292\n",
      "\n",
      "Batch 241/310 ━━━━━━━━━━━━━━━━━━━━ 08:22:13\n",
      "Accuracy: 0.5538 - Loss: 1.2288\n",
      "\n",
      "Batch 242/310 ━━━━━━━━━━━━━━━━━━━━ 08:23:01\n",
      "Accuracy: 0.5538 - Loss: 1.2288\n",
      "\n",
      "Batch 243/310 ━━━━━━━━━━━━━━━━━━━━ 08:23:49\n",
      "Accuracy: 0.5543 - Loss: 1.2283\n",
      "\n",
      "Batch 244/310 ━━━━━━━━━━━━━━━━━━━━ 08:24:37\n",
      "Accuracy: 0.5548 - Loss: 1.2279\n",
      "\n",
      "Batch 245/310 ━━━━━━━━━━━━━━━━━━━━ 08:25:26\n",
      "Accuracy: 0.5548 - Loss: 1.2272\n",
      "\n",
      "Batch 246/310 ━━━━━━━━━━━━━━━━━━━━ 08:26:13\n",
      "Accuracy: 0.5548 - Loss: 1.2278\n",
      "\n",
      "Batch 247/310 ━━━━━━━━━━━━━━━━━━━━ 08:27:01\n",
      "Accuracy: 0.5548 - Loss: 1.2272\n",
      "\n",
      "Batch 248/310 ━━━━━━━━━━━━━━━━━━━━ 08:27:49\n",
      "Accuracy: 0.5551 - Loss: 1.2266\n",
      "\n",
      "Batch 249/310 ━━━━━━━━━━━━━━━━━━━━ 08:28:37\n",
      "Accuracy: 0.5552 - Loss: 1.2262\n",
      "\n",
      "Batch 250/310 ━━━━━━━━━━━━━━━━━━━━ 08:29:25\n",
      "Accuracy: 0.5556 - Loss: 1.2258\n",
      "\n",
      "Batch 251/310 ━━━━━━━━━━━━━━━━━━━━ 08:30:13\n",
      "Accuracy: 0.5560 - Loss: 1.2250\n",
      "\n",
      "Batch 252/310 ━━━━━━━━━━━━━━━━━━━━ 08:31:01\n",
      "Accuracy: 0.5558 - Loss: 1.2258\n",
      "\n",
      "Batch 253/310 ━━━━━━━━━━━━━━━━━━━━ 08:31:49\n",
      "Accuracy: 0.5558 - Loss: 1.2259\n",
      "\n",
      "Batch 254/310 ━━━━━━━━━━━━━━━━━━━━ 08:32:37\n",
      "Accuracy: 0.5562 - Loss: 1.2248\n",
      "\n",
      "Batch 255/310 ━━━━━━━━━━━━━━━━━━━━ 08:33:33\n",
      "Accuracy: 0.5560 - Loss: 1.2251\n",
      "\n",
      "Batch 256/310 ━━━━━━━━━━━━━━━━━━━━ 08:34:23\n",
      "Accuracy: 0.5559 - Loss: 1.2251\n",
      "\n",
      "Batch 257/310 ━━━━━━━━━━━━━━━━━━━━ 08:35:11\n",
      "Accuracy: 0.5562 - Loss: 1.2246\n",
      "\n",
      "Batch 258/310 ━━━━━━━━━━━━━━━━━━━━ 08:36:01\n",
      "Accuracy: 0.5567 - Loss: 1.2237\n",
      "\n",
      "Batch 259/310 ━━━━━━━━━━━━━━━━━━━━ 08:36:49\n",
      "Accuracy: 0.5571 - Loss: 1.2228\n",
      "\n",
      "Batch 260/310 ━━━━━━━━━━━━━━━━━━━━ 08:37:37\n",
      "Accuracy: 0.5569 - Loss: 1.2232\n",
      "\n",
      "Batch 261/310 ━━━━━━━━━━━━━━━━━━━━ 08:38:26\n",
      "Accuracy: 0.5572 - Loss: 1.2231\n",
      "\n",
      "Batch 262/310 ━━━━━━━━━━━━━━━━━━━━ 08:39:14\n",
      "Accuracy: 0.5571 - Loss: 1.2232\n",
      "\n",
      "Batch 263/310 ━━━━━━━━━━━━━━━━━━━━ 08:40:03\n",
      "Accuracy: 0.5575 - Loss: 1.2224\n",
      "\n",
      "Batch 264/310 ━━━━━━━━━━━━━━━━━━━━ 08:40:52\n",
      "Accuracy: 0.5573 - Loss: 1.2228\n",
      "\n",
      "Batch 265/310 ━━━━━━━━━━━━━━━━━━━━ 08:41:40\n",
      "Accuracy: 0.5580 - Loss: 1.2223\n",
      "\n",
      "Batch 266/310 ━━━━━━━━━━━━━━━━━━━━ 08:42:28\n",
      "Accuracy: 0.5583 - Loss: 1.2221\n",
      "\n",
      "Batch 267/310 ━━━━━━━━━━━━━━━━━━━━ 08:43:17\n",
      "Accuracy: 0.5583 - Loss: 1.2223\n",
      "\n",
      "Batch 268/310 ━━━━━━━━━━━━━━━━━━━━ 08:44:06\n",
      "Accuracy: 0.5584 - Loss: 1.2220\n",
      "\n",
      "Batch 269/310 ━━━━━━━━━━━━━━━━━━━━ 08:44:54\n",
      "Accuracy: 0.5587 - Loss: 1.2212\n",
      "\n",
      "Batch 270/310 ━━━━━━━━━━━━━━━━━━━━ 08:45:43\n",
      "Accuracy: 0.5588 - Loss: 1.2207\n",
      "\n",
      "Batch 271/310 ━━━━━━━━━━━━━━━━━━━━ 08:46:31\n",
      "Accuracy: 0.5589 - Loss: 1.2204\n",
      "\n",
      "Batch 272/310 ━━━━━━━━━━━━━━━━━━━━ 08:47:19\n",
      "Accuracy: 0.5588 - Loss: 1.2206\n",
      "\n",
      "Batch 273/310 ━━━━━━━━━━━━━━━━━━━━ 08:48:08\n",
      "Accuracy: 0.5595 - Loss: 1.2196\n",
      "\n",
      "Batch 274/310 ━━━━━━━━━━━━━━━━━━━━ 08:48:56\n",
      "Accuracy: 0.5596 - Loss: 1.2202\n",
      "\n",
      "Batch 275/310 ━━━━━━━━━━━━━━━━━━━━ 08:49:45\n",
      "Accuracy: 0.5593 - Loss: 1.2206\n",
      "\n",
      "Batch 276/310 ━━━━━━━━━━━━━━━━━━━━ 08:50:33\n",
      "Accuracy: 0.5591 - Loss: 1.2210\n",
      "\n",
      "Batch 277/310 ━━━━━━━━━━━━━━━━━━━━ 08:51:22\n",
      "Accuracy: 0.5588 - Loss: 1.2208\n",
      "\n",
      "Batch 278/310 ━━━━━━━━━━━━━━━━━━━━ 08:52:11\n",
      "Accuracy: 0.5589 - Loss: 1.2205\n",
      "\n",
      "Batch 279/310 ━━━━━━━━━━━━━━━━━━━━ 08:52:59\n",
      "Accuracy: 0.5591 - Loss: 1.2202\n",
      "\n",
      "Batch 280/310 ━━━━━━━━━━━━━━━━━━━━ 08:53:48\n",
      "Accuracy: 0.5596 - Loss: 1.2194\n",
      "\n",
      "Batch 281/310 ━━━━━━━━━━━━━━━━━━━━ 08:54:36\n",
      "Accuracy: 0.5595 - Loss: 1.2201\n",
      "\n",
      "Batch 282/310 ━━━━━━━━━━━━━━━━━━━━ 08:55:24\n",
      "Accuracy: 0.5594 - Loss: 1.2207\n",
      "\n",
      "Batch 283/310 ━━━━━━━━━━━━━━━━━━━━ 08:56:13\n",
      "Accuracy: 0.5593 - Loss: 1.2206\n",
      "\n",
      "Batch 284/310 ━━━━━━━━━━━━━━━━━━━━ 08:57:01\n",
      "Accuracy: 0.5599 - Loss: 1.2198\n",
      "\n",
      "Batch 285/310 ━━━━━━━━━━━━━━━━━━━━ 08:57:50\n",
      "Accuracy: 0.5601 - Loss: 1.2195\n",
      "\n",
      "Batch 286/310 ━━━━━━━━━━━━━━━━━━━━ 08:58:41\n",
      "Accuracy: 0.5608 - Loss: 1.2188\n",
      "\n",
      "Batch 287/310 ━━━━━━━━━━━━━━━━━━━━ 08:59:30\n",
      "Accuracy: 0.5609 - Loss: 1.2183\n",
      "\n",
      "Batch 288/310 ━━━━━━━━━━━━━━━━━━━━ 09:00:18\n",
      "Accuracy: 0.5608 - Loss: 1.2181\n",
      "\n",
      "Batch 289/310 ━━━━━━━━━━━━━━━━━━━━ 09:01:06\n",
      "Accuracy: 0.5607 - Loss: 1.2182\n",
      "\n",
      "Batch 290/310 ━━━━━━━━━━━━━━━━━━━━ 09:01:55\n",
      "Accuracy: 0.5608 - Loss: 1.2178\n",
      "\n",
      "Batch 291/310 ━━━━━━━━━━━━━━━━━━━━ 09:02:43\n",
      "Accuracy: 0.5602 - Loss: 1.2185\n",
      "\n",
      "Batch 292/310 ━━━━━━━━━━━━━━━━━━━━ 09:03:32\n",
      "Accuracy: 0.5603 - Loss: 1.2189\n",
      "\n",
      "Batch 293/310 ━━━━━━━━━━━━━━━━━━━━ 09:04:20\n",
      "Accuracy: 0.5604 - Loss: 1.2190\n",
      "\n",
      "Batch 294/310 ━━━━━━━━━━━━━━━━━━━━ 09:05:08\n",
      "Accuracy: 0.5605 - Loss: 1.2189\n",
      "\n",
      "Batch 295/310 ━━━━━━━━━━━━━━━━━━━━ 09:05:57\n",
      "Accuracy: 0.5605 - Loss: 1.2188\n",
      "\n",
      "Batch 296/310 ━━━━━━━━━━━━━━━━━━━━ 09:06:45\n",
      "Accuracy: 0.5608 - Loss: 1.2182\n",
      "\n",
      "Batch 297/310 ━━━━━━━━━━━━━━━━━━━━ 09:07:33\n",
      "Accuracy: 0.5603 - Loss: 1.2191\n",
      "\n",
      "Batch 298/310 ━━━━━━━━━━━━━━━━━━━━ 09:08:21\n",
      "Accuracy: 0.5601 - Loss: 1.2191\n",
      "\n",
      "Batch 299/310 ━━━━━━━━━━━━━━━━━━━━ 09:09:09\n",
      "Accuracy: 0.5599 - Loss: 1.2191\n",
      "\n",
      "Batch 300/310 ━━━━━━━━━━━━━━━━━━━━ 09:09:59\n",
      "Accuracy: 0.5597 - Loss: 1.2195\n",
      "\n",
      "Batch 301/310 ━━━━━━━━━━━━━━━━━━━━ 09:10:46\n",
      "Accuracy: 0.5597 - Loss: 1.2193\n",
      "\n",
      "Batch 302/310 ━━━━━━━━━━━━━━━━━━━━ 09:11:36\n",
      "Accuracy: 0.5595 - Loss: 1.2194\n",
      "\n",
      "Batch 303/310 ━━━━━━━━━━━━━━━━━━━━ 09:12:25\n",
      "Accuracy: 0.5598 - Loss: 1.2191\n",
      "\n",
      "Batch 304/310 ━━━━━━━━━━━━━━━━━━━━ 09:13:13\n",
      "Accuracy: 0.5596 - Loss: 1.2192\n",
      "\n",
      "Batch 305/310 ━━━━━━━━━━━━━━━━━━━━ 09:14:02\n",
      "Accuracy: 0.5595 - Loss: 1.2195\n",
      "\n",
      "Batch 306/310 ━━━━━━━━━━━━━━━━━━━━ 09:14:50\n",
      "Accuracy: 0.5604 - Loss: 1.2181\n",
      "\n",
      "Batch 307/310 ━━━━━━━━━━━━━━━━━━━━ 09:15:39\n",
      "Accuracy: 0.5602 - Loss: 1.2184\n",
      "\n",
      "Batch 308/310 ━━━━━━━━━━━━━━━━━━━━ 09:16:27\n",
      "Accuracy: 0.5602 - Loss: 1.2183\n",
      "\n",
      "Batch 309/310 ━━━━━━━━━━━━━━━━━━━━ 09:17:15\n",
      "Accuracy: 0.5607 - Loss: 1.2176\n",
      "\n",
      "Batch 310/310 ━━━━━━━━━━━━━━━━━━━━ 09:18:04\n",
      "Accuracy: 0.5610 - Loss: 1.2172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, LayerNormalization, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "num_classes = 6\n",
    "image_size = (224, 224)\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# ConvNeXt Model\n",
    "def create_convnext_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # ConvNeXt Backbone\n",
    "    convnext = ConvNeXtBase(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "    convnext.trainable = False\n",
    "    x = convnext(inputs)\n",
    "    \n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Simplified Metrics Callback\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches):\n",
    "        super().__init__()\n",
    "        self.total_batches = total_batches\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {batch+1}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Loss: {loss:.4f}\\n\")\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (224, 224, 3)\n",
    "model = create_convnext_model(input_shape, num_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "csv_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\FETAL_PLANES_DB_data.csv\"\n",
    "df = pd.read_csv(csv_path, delimiter=\";\")\n",
    "\n",
    "# Shuffle the DataFrame for a random split\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Add .png extension to each image name\n",
    "df[\"Image_name\"] = df[\"Image_name\"].apply(lambda x: f\"{x}.png\")\n",
    "\n",
    "# Image data generator with train-validation split\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\Images\",\n",
    "    x_col=\"Image_name\",\n",
    "    y_col=\"Plane\",\n",
    "    target_size=image_size,\n",
    "    class_mode=\"sparse\",\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\Images\",\n",
    "    x_col=\"Image_name\",\n",
    "    y_col=\"Plane\",\n",
    "    target_size=image_size,\n",
    "    class_mode=\"sparse\",\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training with Metrics Callback\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, MetricsCallback(total_batches=len(train_gen))],\n",
    "    verbose=0  # Set verbose to 0 to avoid duplicate output\n",
    ")\n",
    "\n",
    "# Evaluate on Validation Data\n",
    "val_loss, val_accuracy = model.evaluate(val_gen)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Calculate Metrics on Validation Set\n",
    "val_preds = model.predict(val_gen)\n",
    "val_labels = val_gen.classes\n",
    "\n",
    "# Convert predictions to label format\n",
    "val_preds = np.argmax(val_preds, axis=1)\n",
    "\n",
    "precision = precision_score(val_labels, val_preds, average='weighted')\n",
    "recall = recall_score(val_labels, val_preds, average='weighted')\n",
    "f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualization of Training History\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fc46f-92f4-4d83-9c13-185b81a1724d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
